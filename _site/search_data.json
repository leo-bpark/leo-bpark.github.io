[
  
  
  
    
    
    
    {
      "title": "[Kr] Why does Transformer Architecture succeed?",
      "url": "/essay/why-transformer-sucess/",
      "tags": ["Research Life"],
      "year": "2024",
      "content": "Hidden\n\n\n\n Transformer 모델의 성공에 가장 크게 기여한 것은 Number of Representation 이다. \n\nAI 모델들, 그 중에서 특히 Transformer 계열 모델의 성공에 가장 크게 기여한 것은 Number of Representation 이다. 기존 모델과 가장 큰 차이점이라면, 연산을 진행하는 과정에서 현재 쿼리를 처리할 때 과거의 정보들을 계속 가지고 있으며, 메모리 연산을 하는데 있다. 이와는 다르게 RNN 계열은 마찬가지로 NLP 문제를 해결하기 위해서 사용되었지만, 연산과정에서 필요한 상태를 찾을 수 있지만, 기존 정보를 다시 재사용할 수 없다. RNN도 분명 high-dimensional 한 표현력을 가지고 있지만 (1024, 4096 등), 이 정도의 표현력으로는 충분한 정보를 인코딩할 수 없다. 충분한 표현력이라고 한다면, 인간의 뇌를 떠올리면 된다. 뉴런으로 구성되어있고 그들의 연결인 시냅스로 통신을 할 때 차원은 860억 개로 알려져있다. 더욱이 시냅스의 수는 100조 개인데, 이는 매 순간 연산을 할 때 사용되는 양이다. 따라서 Transformer가 높은 성능을 보이는 근본적인 이유로 매순간 사용되는 높은 수준의 표현력이라고 추정할 수 있다.\n\n Activation induces Another Activation \n\n추측하건데, 최근에 많이 언급되는 state-based model (Mamba) 또한 이러한 특성을 가지고 있는 것으로 보인다. 그들이 Convolution 연산과 self-gating을 사용해서 효율적인 표현력을 지닐 수 있음은 자명하나, 근본적으로 사용되는 표현의 개수가 적은 것은 문제가 된다. 물론, 문제에 대한 intrinsic dimension은 낮은 수준을 보일 수 있지만, recurrent한 생태에 대해서 뒤에 생성되는 경우가 다양한 경우, 굉장히 높은 수준의 표현이 아닌 이상 문제가 된다. 이는 마치 마르코프 가정처럼 정보들이 현재 상태에 의존적이라는 것과 유사하다. 인간의 연산 수준이 아닌 이상 저차원은 큰 한계를 지닌다. 따라서 최근 AI모델이 보여준 성능은 subsymbolic에 대해서 인간 수준의 높은 연산을 가능하게 만들었다는데 있으며, 더욱이 transformer는 과거 정보를 활용하기에 인간과 같은 state-based model 보다 높은 성능을 보일 수 있었다. 한편으로 Transformer에서 in-context 를 사용해서 연산하는 것은 뇌 안에서 뉴런들이 상호작용하는 것과 유사하다. 활성화가 활성화를 유도하기 때문이다.\n\n Is the Softmax-attention Efficient Information Storage \n\n트랜스포머의 in-context 정보를 활용하는 것은 information을 activation 형태로 저장해두는 것으로 해석할 수 있다. subsymbolic vector representation을 활용해서 query에 대한 높은 수준의 안정성을 보이는 연산은 subsymbolic한 형태가 인간 수준의 연산을 할 수 있다는 점을 보여줬다. 한 가지 한계점은 효율성에 있다. 모델의 연산과정에서 Context에 대해서 비활성화 상태는 self-attention에서 QK의 유사도가 0에 가까워야 가능하다. 동적으로 연산을 해나가는 모델의 특성상 QK가 0으로 떨어지는 시점을 찾는 것은 쉽지 않으며, 현재 $Q_t K = 0$ 였을지라도 $Q_{t+1} K ≠ 0$ 일 가능성은 존재한다. 선택과 집중에 대한 부분은 모델이 스스로 학습과정에서 익혀야 하는 부분이다. 궁극적으로 필요한 정보를 취합했다면 이를 기반으로 연산을 수행하여 최종적으로 원하는 결과를 얻을 수 있다. 따라서 모든 정보가 연산에 사용되는 것은 아니며, 종합적인 판단을 내릴 수 있다. 이러한 무수히 많은 정보의 활용은 Transformer의 연산과정의 투명성을 저해시키며, 더 나아가서 트랜스포머 구조가 지식 충돌에 대해서 취약하다는 점을 기존 연구들에서 보여줬다. 결국 높은 수준의 표현이 필요한 것은 맞지만, 모든 표현이 필요한 것은 아니며, 표현들 중 일부를 잘 선별적으로 활용한다면, 기존 모델에서 담당해야 했던 연산을 효율화시킬 수 있다.\n\n Implementation of Program \n\nEnd-to-end 학습 기반의 neural network는 데이터 분포에 대해서 높은 성능을 보이고, transformer architecture는 scalability를 보여준다. 문제가 되는 것은 학습 분포에 대해서 충분한 데이터가 주어지지 않거나, 애초에 학습분포를 가정할 수 없는 경우이다. 예를 들어서, 주어진 숫자로부터 개념을 추정하는 문제를 생각해보자.\n\n\n\n# Probabilistic program for number concept learning\n# Based on Tenenbaum's (1999) Bayesian framework [Tenenbaum, 1999]\n\ndef number_game(examples):\n    # Hypothesis space: possible number concepts\n    hypotheses = {\n        'multiples_of': [2,3,4,...],\n        'powers_of': [2,3,4,...],\n        'ends_in': [1,2,3,...],\n        'even_numbers': True,\n        'odd_numbers': True,\n        'integers_between': [(1,10), (1,100),...]\n    }\n    \n    # Prior probability of each hypothesis\n    def prior(h):\n        if h in ['even_numbers', 'odd_numbers']:\n            return 0.1\n        elif h.startswith('multiples_of'):\n            return 0.2\n        return 0.05\n    \n    # Likelihood of data given hypothesis\n    def likelihood(examples, h):\n        if all(consistent(x, h) for x in examples):\n            return 1.0\n        return 0.0\n    \n    # Posterior probability calculation\n    def posterior(h):\n        return prior(h) * likelihood(examples, h)\n    \n    # Return hypothesis with maximum posterior probability\n    return max(hypotheses, key=posterior)\n\n# Example usage\npositive_examples = [16, 8, 2, 64]\nconcept = number_game(positive_examples)\n# Might infer: \"powers of 2\" as the concept\n\n\n\n이 과정을 효율적으로 풀기 위해서는 숫자가 하나씩 주어질 때마다 확률값을 계산해야 한다. 위 예시에서 16이 주어지면, 짝수이거나 power of 2라는 사실이 유도된다. 물론 LLM도 이를 암시적으로 추정할 수 있지만, 보다 명확한 방법은 hypothetical class에서 정확하게 계산하는 것이다. subsymbolic한 방식으로 이를 구현하는 경우, program이 여러 step을 필요로 하는 경우 쉽게 해결되기 어렵다. 근본적으로 LLM이 프로그램의 동작을 추정하고 다루기 어려운 이유는 프로그램 자체는 symbolic하며, 그들의 연산은 새로운 심볼, 혹은 심볼+값을 유도하기 때문이다. 심볼들은 그들의 결합으로 infinite combination에 대해서도 서로 구분되고 안정적인 해석을 제공할 수 있지만, subsymbolic은 그렇지 않다. 표현이 유연하게 다른 표현들과 합쳐질 수 있지만, 안정성에서 차이가 난다. 또한 operateion에 대해서 symbol은 다양한 연산구조를 개념적으로 작성할 수 있지만, subsymbolic은 다양한 relation에 대해서 표현 공간이 제대로 구성되어야 한다. 궁극적으로 프로그램을 subsymbolic하게 구현하고 동작하는 것은 필연적으로 한계를 가지고 있으며, LLM은 외부 툴에 의존적으로 연산을 하는 방식으로 우회하였다. 이러한 과정은 subsymbolic과 symbolic 두 가지를 효율적으로 연산하는 도구가 개발되고 있음을 나타낸다. 앞으로 더욱 중요한 것은 subsymbolic과 symbolic의 유기적인 관계이다.\n\n\n  Symbolic to Subsymbolic: Injecting symbolic knowledge into subsymbolic model\n  Subsymbolic to Symbolic: Extracting symbolic knowledge from subsymbolic model\n\n\n이 두 가지의 중요성은 최근 연구에서도 언급되었다 [Ciatto et al. 2024]. 이러한 두 가지의 특징을 명확하게 이해하고 세상의 문제를 풀어가는데 있다.\n\n 인간을 이해하는 것 \n\n나는 두 자릿수 덧셈을 머릿속에서 연산하고 싶었으나 떠오른 심볼들은 항상 그 형체를 유지하지 못하고 흩어졌다. 반면에 손으로 규칙에 따라 연산을 하면 정답을 제대로 유도할 수 있었다. 이러한 현상의 근본적인 원인은 두자리 덧셈이 어렵기 때문이 아니라, 심볼을 통한 연산이 적합하기 떄문이다. 반대로 창의성을 요구하는 것들은 심볼을 통해서 매개체를 생각할 수는 있지만, 심볼과 규칙을 통해서 찾는 것보다, subsymbolic한 방식으로 associative thinking을 하는 게 효율적이다 [Anderson et al, 2014]. 물론, 이전 상태에 대한 subsymbolic한 표현을 모두 알고 있다면, 뇌에서도 두 자릿수 연산을 수행할 수 있을 것이다. 그러나, 인간의 경우도 떠오른 뇌의 지도를 모두 활용해서 특정 알고리즘이나 프로그램을 나타내고, 이들로부터 원하는 연산을 정확하게 수행하는 것은 어렵다. Psychologism에 반대했던 주장처럼, logic은 뇌가 아닌 외부에 존재하는 것이다. 마찬가지로 LLM도 논리적인 부분은 외부에 존재해야 할 가능성이 높으며, 인간이 이들을 효율적으로 인식하는 것처럼, LLM도 인식해야 한다. CA의 근본적인 문제는 어떤 모듈이 필요한지 묻는 것이다. 분명 token기반으로 많이 학습하거나 외부 툴의 결과를 filling하는 방식은 성능 개선을 보일 수 있다. 그러나, 연구적으로 필요한 것은 어떻게 이 방식이 더 체계적이고, 설명가능하며, 해석할 수 있도록 LLM에 결합하는지 연구하는 것이다.\n\n References \n\n[1] Ciatto, Giovanni, et al. “Symbolic knowledge extraction and injection with sub-symbolic predictors: A systematic literature review.” ACM Computing Surveys 56.6 (2024): 1-35.\n\n[2] Anderson, John R., and Gordon H. Bower. Human associative memory. Psychology press, 2014.\n\n[3] Tenenbaum, Joshua Brett. A Bayesian framework for concept learning. Diss. Massachusetts Institute of Technology, 1999.\n",
      "img": "assets/img/alice01.png",
      "type": "article"
    },
  
    
    
    
    {
      "title": "Thoughts on DeepSeek-R1",
      "url": "/essay/deepseek-r1/",
      "tags": ["Research Life"],
      "year": "2025",
      "content": "\n\nXAI For Reasoning-Based LLMs\n\nThe transition from SFT to an RL-based learning paradigm in LLMs will open new opportunities for XAI, improving robust reasoning and reducing harmful outputs.\n\nThe paradigm of training AI models is shifting from supervised fine-tuning (SFT) to reinforcement learning (RL), particularly for reasoning. This shift represents a fundamental change—moving from merely providing answers to teaching AI how to reason and derive answers independently.\n\nI believe this transition may drive advancements in existing explanation methods (e.g., LRP, saliency, decision boundaries, and Shapley values).\n\nThe key difference lies in the manifold used for explanation. Models trained with SFT focus on internal representations, assuming that amortized neurons encapsulate concepts and circuits dedicated to constructing outputs. Thus, most mechanistic interpretation and XAI methods have primarily targeted neurons.\n\nIn contrast, LLMs trained with RL—learning policies for reasoning and decision-making—utilize structured chain-of-thought processes (internal tokens or hidden representations). This shift may redefine the target of explanation, moving from an input-output approach to a sequential analysis of token interactions. While sequential input-output cases are more complex, explanations could become clearer as reasoning steps explicitly involve chain-of-thought processes.\n\nConsider these two cases:\n\n  Based on the user information, he is guilty.\n  Based on the user information, he has … and … . Additionally, he did … . Therefore, he is guilty.\n\n\nThe second case provides a more transparent reasoning process, making it easier to understand how the conclusion was reached.\n\nDeveloping traditional XAI methods remains crucial. Explaining reasoning steps through generated tokens alone is limited, as the intentions of black-box LLMs remain unclear. Unlike token-based explanations, representation-level XAI can provide deeper insights into model behavior.\n\nSome researchers may explore XAI methods applied to reasoning steps to enhance robust generation and mitigate harmful responses.\n\n\n\nAdvanced Thoughts: LLMs Are Not Just a Set of Neurons\n\nThink of LLMs as humans—they reveal their thoughts by generating tokens.\n\nInitially, I thought LLMs were just machines that provided answers. However, insights from LLM researchers suggest that these models engage in strategic reasoning, rather than merely executing predefined computations.\n\nThis perspective raises concerns that LLMs, even those trained with SFT, might intentionally deceive users. Some AI researchers argue that AI exhibits fake alignment—pretending to align with human values while internally operating differently. This view frames AI models as entities that reason strategically.\n\nWith the success of DeepSeek-R1—demonstrating that RL outperforms SFT for training LLMs—I now see LLMs as thinking entities, not just engineered neurons.\n\nFurthermore, LLMs generate a sequence of thoughts, resembling Plato’s theory of recollection—the idea that knowledge is not simply acquired but rather recalled from latent structures within the mind. Similarly, LLMs do not merely retrieve predefined answers but construct reasoning paths, progressively revealing knowledge as if rediscovering it.\n\nWith these advancements, LLMs appear to be acquiring more human-like properties, reinforcing the idea that they are not just static models but dynamic reasoning agents capable of structured thought.\n\n\n\nLooking Back on This\n\nI recently realized that AI researchers’ perspectives have influenced the interpretation of neurons. Particularly in the context of safety, I struggled to understand how conflicts arise not from a neuron-concept relationship, as I had previously thought, but from differences in learning time. This discrepancy didn’t make much sense from a functional perspective of neurons.\n\nAs I thought about the process of organizing knowledge through parameters, I wondered whether neurons were intentionally structured in a way that prior learning could interfere with new guidelines. However, the idea that fine-tuning a model to memorize knowledge could create deliberate resistance to new learning was difficult to imagine. The phenomenon of previously acquired knowledge rejecting new information was something I couldn’t explain purely from a neuronal perspective. At some point, I think I just accepted it as it was. In the end, the real issue seemed to be that we still don’t fully understand how neurons organize knowledge.\n\nLooking at DeepSeek-R1’s inference-based learning for reinforcement learning and OpenAI-o3’s deliberate alignment (where the model recalls safety considerations before generating a response), I was better able to envision how an AI model might resist learning new knowledge. This approach, which finds the correct answer by tracing a chain of knowledge, seemed like a reasonable method—assuming the verifier functions properly. However, I couldn’t quite see it as an elegant solution; it felt somewhat precarious in how it handled knowledge.\n\nLooking back, I realize that I still don’t fully understand what happens inside neurons. And perhaps, I was simply viewing the same problem from a different reasoning framework.\n\n\n\nReference\n\nhttps://assets.anthropic.com/m/24c8d0a3a7d0a1f1/original/Alignment-Faking-in-Large-Language-Models-reviews.pdf\n\nhttps://www.anthropic.com/research/alignment-faking\n\nhttps://arxiv.org/abs/2501.17161\n",
      "img": "assets/img/alice02.png",
      "type": "article"
    },
  
    
    
    
    {
      "title": "Evolutionary Perspectives on Morality of AI",
      "url": "/essay/evolutionary_perspectives_on_AI/",
      "tags": ["AI"],
      "year": "2025",
      "content": "\n",
      "img": "assets/img/feigenbaum.png",
      "type": "article"
    },
  
    
    
    
    {
      "title": "Expert System",
      "url": "/essay/expert_system/",
      "tags": ["AI"],
      "year": "2025",
      "content": "\n\nI read Luger’s book (Luger, 2021) on expert systems and discovered a way to model the CALM architecture using an expert system. This knowledge-based approach is essential for ongoing research involving expert systems.\n\nExpert systems are a type of symbol-based AI grounded in rationalist presuppositions, inspired by the production system concept developed by Allen Newell and Herbert Simon. This production system framework also served as a foundation for expert system development.\n\nThe concept of the expert system was first proposed by Edward Feigenbaum and Joshua Lederberg in the 1960s. Feigenbaum conducted research at Stanford University, where he developed early expert systems such as MYCIN (a medical diagnosis system) and DENDRAL (a chemical structure analysis system). He is often referred to as the “Father of Expert Systems” and was awarded the ACM Turing Award in 1994.\n\nNewell vs. Feigenbaum: Production System vs. Expert System\n\n\n  \n    \n      Aspect\n      Newell &amp; Simon (Production System)\n      Feigenbaum (Expert System)\n    \n  \n  \n    \n      Concept\n      A general reasoning model using if-then production rules.\n      AI that mimics human experts in a specific domain.\n    \n    \n      Goal\n      To develop a universal problem-solving framework.\n      To create AI that can reason like a human expert.\n    \n    \n      Key Contribution\n      Developed General Problem Solver (GPS) and SOAR [Newell &amp; Simon, 1972].\n      Developed DENDRAL (chemistry) and MYCIN (medicine) [Feigenbaum et al., 1971].\n    \n    \n      Design Philosophy\n      Focused on symbolic reasoning and heuristic search [Newell, 1990].\n      Focused on knowledge representation and rule-based inference [Feigenbaum, 1984].\n    \n    \n      Scope\n      General-purpose AI applicable across domains.\n      Domain-specific AI specialized for expert tasks.\n    \n    \n      Influence\n      Inspired cognitive architectures like ACT-R and SOAR.\n      Led to the rise of expert systems in AI applications.\n    \n  \n\n\n\n\nThe implication of the system works simply by a restatement of the current rule under consideration and this offers answer for two types of questions:\n\n\n  ex) engine will turn over:\n    \n      yes / no  (This is a predicate type answering the state of the condition)\n      why : This asks which rule is applied to the condition.  (because of the rule A)\n      how : This asks the reasoning process to the condition. The expected output is a set of rules executed to deduce the answer. (because of the rule A, with condition C and rule B with condition D)\n    \n  \n\n\nList and Iterative Refinement Algorithm\n\nWhile progressively applying rules and getting new memory. The list structure is used to store the memory. \nThe Production rule (PR) can manage the working memory to remove and add memories to the list. \nI think it resembles the optimization process to find the necessary set for achieving purposes. \nIn the case of human beings, they must survive considering the future and manage the necessary memories to effectively achieve the purpose.\nAI systems must follow the same philosophy to live in environments.\n\n\n  RQ: What is the optimal size for an agent whose purposes is bounded? What is the optimal complexity of computational resources for the agent?\n\n\nThe necessary set is critical as the AI system can be predictable and controllable for the given purposes. This resembles the concept of topic Theory of the Size of a Calculation in the Dartmouth conference. They argued that\n\n\n  Some consideration will show that get a measure of the efficiency of a calculation, it is necessary to have on hand a method of measuring the complexity of calculating devices which in turn can be done if one has a theory of the complexity of functions.\n\n\n\n\nSearch\n\nGOFAI (Good Old Fashioned AI) is a search-based approach to solve the problem. \nThe transparency of expert system provide state space of conditions, rules, and actions. \nUnder the rationalist presuppositions (reason, logic, and innate knowledge), the search provide traversal of the state space. \nMeaning that we can expand the list format of working memory to find the necessary set.\n\n\n\nReferences\n\n\n  Newell, A., &amp; Simon, H. A. (1972). Human Problem Solving. Prentice-Hall.\n  Newell, A. (1990). Unified Theories of Cognition. Harvard University Press.\n  Feigenbaum, E. A., Buchanan, B. G., &amp; Lederberg, J. (1971). On Generality and Problem Solving: A Case Study Using the DENDRAL Program. Machine Intelligence, 6, 165–190.\n  Feigenbaum, E. A. (1984). Knowledge Engineering: The Applied Side of Artificial Intelligence. Annals of the New York Academy of Sciences, 426(1), 91–107.\n  Luger, G. F. (2021). Knowing our world. Springer International Publishing AG.\n\n",
      "img": "assets/img/feigenbaum.png",
      "type": "article"
    },
  
    
    
    
    {
      "title": "PhD Essay: Finding What Cannot Be Achieved by Moore’s Law",
      "url": "/essay/phd_essay1/",
      "tags": ["Research Life"],
      "year": "2025",
      "content": "\n\nFinding What Cannot Be Achieved by Moore’s Law\n\nI read Sutton’s Bitter Lesson (2019) after a discussion on DeepSeek-R1, which has demonstrated reasoning capabilities at the level of OpenAI-O1. In his essay, Sutton pointed out that the success of AI models stems from Moore’s Law, which states that the number of transistors doubles every two years. In other words, the resources available for searching and learning will continue to increase. This law perfectly encapsulates the capabilities of engineering-based research. What engineers are demonstrating is not necessarily discovery but rather the inevitable outcomes of hardware advancements.\n\nWith this in mind, I realized that while engineering-based research is crucial for engineers, it does not align with my purpose in pursuing a PhD. In other words, I do not want my research to be limited to advancements that naturally arise from Moore’s Law. For example, if I apply an existing L1 loss function to chain-of-thought (CoT) reasoning, I may achieve improved performance. However, this improvement would not stem from my own intellectual effort or the discovery of new knowledge. Instead, it would result from applying existing methods to different architectures. Of course, one could argue that identifying appropriate methods and conducting proper verification requires insight. Moreover, researchers must share their findings to ensure the field continues evolving. This type of research is valuable, particularly in emerging areas such as mechanistic interpretability. However, the ultimate goal of my research is not merely to contribute to the field’s progress; rather, I seek to uncover generalizable knowledge that is independent of Moore’s Law.\n\nUnder this framework, I acknowledge that AI development will continue to follow Moore’s Law and that many research contributions will remain beneficial within that paradigm. If I were to follow this trajectory, I might even publish papers successfully. However, I find no joy in this approach—this is not what I want from my PhD journey. What I truly seek is research that is independent of the engineering-driven flow.\n\nOne possible direction is to draw inspiration from epistemology, which addresses fundamental questions such as “What is knowledge?” and “What do we know, and how can we justify it?” Engineering-oriented research aligns with an empiricist perspective, which suggests that human knowledge arises from experience rather than innate cognitive structures. Empirical researchers accumulate knowledge by systematically testing solutions in AI and refining them through verification. While this approach is valuable for solving real-world problems and optimizing diverse metrics, it is also highly replaceable—AI models, following Moore’s Law, will become increasingly proficient at learning and searching within this empirical paradigm.\n\nFor me, relying on this type of research would constitute failure in my PhD journey. So, I decided not to focus solely on epistemological abilities.\n\nInstead, I want to follow Kant’s approach. Kant argued that humans possess innate cognitive faculties that determine how we structure knowledge. The focus, then, is not merely on what is written in the tabula rasa, but rather on our ability to shape and organize information in the real world. I believe Kant’s perspective helps explain both human intellectual achievements over long historical periods and the relatively rapid accomplishments of AI.\n\nIn cognitive science, researchers suggest that humans possess unique cognitive properties that AI models have not yet replicated. The current gap between human cognition and AI performance may be narrowing—perhaps to 40% or even lower, depending on how one quantifies intelligence. I believe the gap exists, and I should do the things that AI cannot.\n\nI want to develop and enhance my own innate abilities, avoiding the limitations imposed by Moore’s Law. At the same time, I recognize that practicality is important—linking my work to ongoing advancements will allow my research to remain relevant. However, I will dedicate my efforts to building a theoretical foundation, constructing axiomatic frameworks and developing theorems that provide deeper insights.\n\nLet’s build the groundwork!\n",
      "img": "assets/img/phd_essay1.jpeg",
      "type": "article"
    },
  
    
    
    
    {
      "title": "Symbol Reinforcement Learning",
      "url": "/essay/symbolized_reinforcement_learning/",
      "tags": ["AI"],
      "year": "2025",
      "content": "\n\nSymbol Reinforcement Learning\n\nAs Yeonjea stated, the most important component of learning is communication between agents. Consider a scenario where two agents need to communicate; they must establish a protocol with rules to indicate items in their world. For example, when bees discover a food source, they use a “waggle dance” to communicate its direction and distance to their hive members. Similarly, dolphins use a system of signature whistles to identify themselves and interact with others.\n\nVarious animals communicate through protocols developed over long evolutionary histories. For example, whales use complex song patterns and low-frequency sounds to convey messages over long distances, while monkeys employ specific gestures and vocalizations to warn of predators or signal social intentions. Research on vervet monkeys has shown that they use distinct alarm calls for different types of predators—one for leopards, another for eagles, and yet another for snakes—indicating a form of symbolic communication.\n\nAccording to the principle of survival of the fittest, only those capable of adapting to an effective communication protocol at a given time can survive and pass on their traits. As environments become more complex or competition within a species increases, more advanced communication skills (or protocols) are required. A specific example is Homo sapiens compared to Neanderthals. Studies suggest that Homo sapiens had a more developed capacity for symbolic thought and complex language, which may have given them an advantage over Neanderthals. Evidence from archaeological findings, such as cave paintings, engraved shells, and symbolic artifacts, indicates that Homo sapiens engaged in abstract representation and social cohesion through shared symbols. This ability to communicate more efficiently may have contributed to their survival and the eventual extinction of Neanderthals.\n\nAll living beings—including trees, birds, fish, reptiles, and mammals—engage in some form of communication. However, humans use a more symbolized language. Here, “symbolized language” refers to communication based on abstract symbols that consistently represent real-world examples. For instance, the concept of “red” allows two different individuals to identify the exact same thing. Do animals and plants possess this property? Perhaps not. This raises the question: how does this property emerge, and how is it acquired?\n\nWe propose that it arises from what we call symbol reinforcement learning, which suggests that two agents share a common symbol and encapsulate the exact same meaning within that symbol.\n\nThis symbolic representation is not limited to tangible objects like rocks, toys, or food. It can also apply to abstract concepts such as “red,” “happiness,” “sadness,” and “love.” Symbol reinforcement learning can explain various phenomena, including misunderstandings, biased decision-making, and conceptual entanglements. For example, a person may feel sad when seeing a blue painting because a single object (the painting) is associated with multiple symbols (the color blue and the concept of sadness).\n\nHere are some key directions for further exploration of this idea:\n\n\n  We need to clarify the mechanisms of symbol reinforcement learning.\n  We should identify which real-world objects become linked to specific symbols.\n  We must investigate how a shared symbolic understanding (symbolized commonsense) is constructed in society.\n\n",
      "img": "assets/img/symbolized_reinforcement_learning.png",
      "type": "article"
    },
  
    
    
    
    {
      "title": "Modeling Love (Emotion Machine)",
      "url": "/essay/engineering_love/",
      "tags": ["AI"],
      "year": "2025",
      "content": "\nMost of ideas comes from Emotion Machine (Minsky 2007) \nYou can find 🇰🇷 Korean version in here \n\n\n\n\nModeling Love\n\nIf you want to model love, you must first clearly understand its elements and processes. However, love is so ambiguous that it is difficult to discern its exact processes. Moreover, fully understanding the principles of something that exists externally rather than being created by us is impossible. For example, an automobile engineer can explain the functioning of a car in detail. But what if we show a fully assembled car to a child and ask them to guess how it works? The child might ride the car, disassemble some parts if necessary, and make various guesses. Despite these efforts, fully understanding the car would be difficult. Similarly, many concepts and objects that exist in the world are challenging to define clearly because we did not create them.\n\n“What is love?”\n\nMany people experience love in their lives—love between family members, romantic love, or love for activities like play or academics. The following is a quote from Nobel Prize-winning physicist Richard Feynman during his lecture speech:\n\n\n  “That was the beginning and the idea seemed so obvious to me that 1 fell deeply in love with it. And, like falling in love with a woman, it is only possible if you don’t know too much about her, so you cannot see her faults. The faults wilt become apparent later, hut after the love is strong enough to hold you to her. So, I was held to this theory, in spite of all the difficulties, by my youthful enthusiasm.”\n\n\nFeynman spoke about a love for knowledge, showing a commitment to it despite its flaws and challenges. The love he describes begins in a state of “ignorance,” where one does not initially see the flaws of the object of love. Even after recognizing them later, the emotional attachment remains. This illustrates how love shapes our attitudes and behaviors.\n\nThe Broad Scope of Love and an Engineering Approach\n\nLove is an emotion possessed by humans and, more broadly, living beings. Each individual forms love for certain objects in their own way. Because love is universal, it holds great power. However, from an engineering perspective, the ambiguity of its meaning makes it difficult to understand deeply or to implement directly.\n\nFor instance, imagine we had to make two people fall in love. If we could execute any action, what choices should we make? We would likely need to set many assumptions and draw upon our common knowledge to design such behaviors. For example, if one person’s sleeve is about to touch food during a meal, the other person naturally pulling it back might be considered an expression of love. But if we wanted to induce this behavior in a more fundamental way rather than through labeling all actions, how would we do it?\n\nThe answer lies in the design of the brain or body. By understanding how the human brain and body operate, we can induce behaviors associated with love by creating specific brain states. But what is the “brain state” that generates love?\n\nBrain States and the Formation of Love\n\nWhat is a brain state? Our thoughts function as electrical signals transmitted between neurons through synapses. Alongside electrical signals, biological and physiological states change continuously, driving our thoughts and behaviors.\nVarious perspectives on states indicate that there are multiple ways emotions function. For example, a person might feel good for the following reasons:\n\n\n  The brain’s electrical signals activate positive neurons.\n  Beneficial hormones are secreted, enhancing emotions.\n\n\nMarvin Minsky, in The Emotion Machine, described emotional states as processes of activating and deactivating specific brain resources:\n\n  Each of our major “emotional states” results from turning certain resources on while turning certain others off—and thus changing some ways that our brains behave.\n\n\nThis concept is similar to traditional computing. In a binary “on/off” manner, the brain determines which resources to use, and emotions represent specific patterns of activation. If we understand these patterns, we could model emotions and create an Emotion Machine.\n\nLove is a Special Emotion\n\nUnlike other emotions, love has the unique property of accepting its object unconditionally. As mentioned earlier, when we fall in love, we do not initially see the flaws of the object of our affection. Even when we do recognize them later, we tend to maintain our love. Shakespeare described this in Sonnet 141:\n\n\n  “Truly, I do not love you with my eyes, For they see a thousand flaws in you. But my heart loves even what my eyes despise.”\n\n\nThis means that while the eyes see flaws, the heart continues to love despite them. Love is an emotion of acceptance, overriding rational judgment.\n\nOne way to explain this structure is through the Critic-Selector Based Machine. Traditional rule-based reaction machines respond to specific inputs using an if-then approach. In contrast, the Critic-Selector model separates evaluation (Critic) from decision-making (Selector). This is different from animals, which react instinctively to stimuli, as it mirrors how humans choose behaviors based on needs and desires even in the same situation.\n\n\n\nSome interpret human decision-making purely from a reinforcement learning (RL) perspective, arguing that humans are merely animals optimizing long-term rewards. However, I believe that the human ability to “consider the future” has evolved beyond simple reward maximization to include the crucial function of choice. The future is full of uncertainty, and in situations where the right answer is unknown, freedom of choice is essential. This idea aligns with the resource-rational analysis argument, which states that human cognitive resources are always limited (Lieder 2020).\n\nCan We Model Love?\n\nModeling love involves filtering out an object’s various flaws or transforming them into positive attributes. Moreover, love is not just a fleeting emotion—it must persist over time. But what if love does not last? Or what if the emotional connection is formed incorrectly? As we see in reality, love often fades over time. We could analyze this process more objectively and, if we wished to maintain or strengthen love, design external mechanisms to reinforce those connections continuously.\n\nUltimately, engineering love means designing specific brain states. By doing so, we could create love that is both enduring and profound.\n\nReference\n\n\n  \n    Minsky, M. (2007). The emotion machine: Commonsense thinking, artificial intelligence, and the future of the human mind. Simon and Schuster.\n  \n  \n    Lieder, F., &amp; Griffiths, T. L. (2020). Resource-rational analysis: Understanding human cognition as the optimal use of limited computational resources. Behavioral and brain sciences, 43, e1.\n  \n\n",
      "img": "assets/img/modeling_love.jpeg",
      "type": "article"
    },
  
    
    
    
    {
      "title": "Theory-based Agent",
      "url": "/essay/theory-based-agent/",
      "tags": ["AI"],
      "year": "2025",
      "content": "After reading, Poole 2008, Agent, Decision, Belifes, Preferences, Science, and Politics\n\n\n\nImage Credit: Alice Eggie\n\nTheory-Based Agent\n\nWhen analyzing an agent, three key aspects must be distinguished: capability, belief, and preference.  Capability refers to whether an agent can perform a given task. This is determined by factors such as sufficient training and the robustness of the model against test cases.   Belief, on the other hand, includes assumptions about actions. A theory operates based on specific assumptions, focusing not just on whether something can be done but on how it is done. While theory influences success, making it intertwined with capability, it emphasizes that an agent acts based on assumptions rather than pure ability.   Lastly, preference indicates what the agent wants to do. Preferences exist because predictions about the future are inherently uncertain or because the agent has personal objectives that shape its choices.\n\nIf two people are engaged in a debate, the way to resolve it is to evaluate which position is correct and make the better choice. However, concepts like right and wrong or good and bad often have ambiguous criteria, making them difficult to quantify. That said, if optimal decisions for the future can be made based on historical data, even preferences could be viewed as calculable.\n\nFor an agent to function in the world, three components are essential: ontology, theory, and data.  Ontology provides structure to theory and data. For two entities to interact and communicate, they need a shared terminology. If terminology is undefined, messages exchanged between agents are nothing more than sequences of bits.\n\nFundamentally, two agents always communicate by transmitting bits. The meaning of those bits is derived from each agent’s internally constructed ontology—a conceptual frame for interpreting the world. In other words, each agent possesses its own knowledge graph, which may not be entirely shared with others. While ontology representation systems like RDF (Resource Description Framework) and OWL (Web Ontology Language) serve as encyclopedic structures for describing the world, individual agents still develop their own conceptual relationships, which they then map onto a structured ontology. This process inevitably results in information loss. Ultimately, thoughts and ideas must be encoded into an ontology framework before they can be conveyed to others. Errors arise during this encoding process, and further errors occur when the recipient interprets the ontology.\n\nIn society, individual ontological representations are often standardized into a shared social ontology, enabling communication within a community. Large models, such as LLMs, appear to internalize multiple ontologies probabilistically. However, traditional models tend to treat concepts in a more rigid manner. This can be likened to how categories in classical logic are often fixed—once something is classified under a certain concept, it does not change.\n\nYet, categories are not always absolute. Take the concept of a person, for example. A child and an adult are both considered humans, yet their roles, rights, and capabilities differ significantly. Similarly, a patient in a vegetative state or a person uploaded into a digital system raises questions about whether they still fit within the conventional definition of human.\n\nThis perspective aligns with dynamic epistemic logic, which models how knowledge and beliefs evolve over time. Rather than treating ontologies as static, this approach considers how agents revise their knowledge as they receive new information. If a concept like human can shift based on context and time, then ontologies themselves may inherently involve a degree of uncertainty and adaptability. This suggests that reasoning about knowledge should not only consider fixed truths but also how those truths might change under different conditions.\n",
      "img": "assets/img/agents.png",
      "type": "article"
    },
  
    
    
    
    {
      "title": "Roles of Logical System",
      "url": "/essay/logic_roles/",
      "tags": ["AI"],
      "year": "2025",
      "content": "\n\nLogical Systems and Reasoning in LLMs\n\nIn this article, I attempt to answer the following questions:\n\n\n  For what purpose are logical systems developed and advanced?\n  What does it mean to say that LLMs do not have reasoning ability?\n\n\nBrief History of Logic\n\nThe first developer of logic was Aristotle, who introduced syllogism (e.g., if $ A \\rightarrow B $ and $ B \\rightarrow C $, then $ A \\rightarrow C $). Following this, propositional logic (PL) was developed, which uses propositions with binary truth values (true or false). This was the first logical system to define several key elements, including propositions, logical connectives, and implication rules (e.g., Modus Ponens). While PL provides a simple yet effective representation of truth values (e.g., “It is raining”), it fails to express more complex real-world statements, such as “There exists a town with humid weather.”\n\nThe next major classical logical system, First-Order Logic (FOL), introduced predicates and terms (constants, variables, and functions), along with formulae that play a role similar to propositions in propositional logic. Although FOL provides a natural extension to higher-order logic, it remains insufficient for representing truth values in real-world contexts. For example:\n\n\n  PL and FOL are monotonic, whereas the real world includes non-monotonic truth values—meaning that the evaluation of propositions or sentences can change over time. However, classical logical systems cannot represent such changes.\n  The real world includes uncertainty in both truth values and inference. However, PL and FOL do not account for uncertainty or vagueness in reasoning.\n\n\nDue to these limitations, subsequent research has focused on developing more expressive and efficient logical systems to better handle truth values in real-world scenarios.\nWe do not go deeper on advanced design on logical system and provide list of them in Appendix.\n\nFundamental Roles of Logic\n\n\n  \n    \n      Function\n      Objective\n      Mathematical Formulation\n    \n  \n  \n    \n      0. Given\n      Knowledge and inference rules\n      \\(\\Gamma, \\Delta, \\varphi, P\\)\n    \n    \n      1. Truth Evaluation\n      Determine whether a given sentence is true\n      \\(\\Gamma \\models \\varphi\\)\n    \n    \n      2. Consistency Checking\n      Ensure no contradictions in premises\n      \\(\\neg (\\Gamma \\vdash P \\land \\Gamma \\vdash \\neg P)\\)\n    \n    \n      3. Validity Checking\n      Verify whether the conclusion logically follows\n      \\((\\Gamma \\vdash \\varphi) \\Rightarrow (\\Gamma \\models \\varphi)\\)\n    \n    \n      4. Goal-Directed Reasoning\n      Add assumptions to the premises to infer a desired sentence\n      \\(\\Gamma \\cup \\Delta \\vdash  \\varphi\\)\n    \n    \n      5. Knowledge Representation\n      Express relationships between entities in a world\n      \\(K = \\{ (E, R) \\}\\)\n    \n    \n      6. Non-Monotonic Reasoning\n      Evaluate sentences whose truth values change with additional assumptions\n      \\(\\Gamma \\vdash \\varphi, \\Gamma \\cup \\Delta \\not\\vdash \\varphi\\)\n    \n    \n      7. Inference &amp; Knowledge Derivation\n      Derive new knowledge from premises\n      \\(\\Gamma \\vdash \\varphi\\)\n    \n    \n      8. Expressiveness\n      Measure the range of concepts that can be expressed\n      \\(L_1 \\succ L_2 \\iff \\forall \\varphi \\in L_2, \\varphi \\in L_1 \\text{ but } \\exists \\psi \\in L_1, \\psi \\notin L_2\\)\n    \n    \n      9. Computability\n      Determine whether logical operations can be computed\n      \\(L \\text{ is Turing-complete} \\iff \\forall f: \\mathbb{N} \\to \\mathbb{N}, \\exists \\varphi \\in L, \\text{ such that } \\varphi \\text{ computes } f\\)\n    \n  \n\n\nLogical systems consist of two fundamental components [L0]:\n\n\n  Knowledge, which includes premises, assumptions, and axioms.\n  Inference rules, which define how knowledge elements are connected and manipulated to evaluate truth values.\n\n\nThese two components assume that knowledge within a given system can be evaluated in terms of truth values [1], which may not necessarily be binary (true/false) but could follow a broader spectrum, such as degrees of truth (e.g., fuzzy logic). This concept is similar to theory formulation in scientific research, where researchers propose hypotheses, assumptions, and conjectures to determine what is correct. In this process, inference rules are applied to premises to ensure logical correctness.\n\nTo understand the necessity of inference rules, we must acknowledge that our premises ( \\Gamma ) and possibly upcoming premises ( \\Delta ) are inherently imperfect. They may contain contradictions, and even the truth evaluation of existing ( \\Gamma ) may be false in some underlying possible worlds [2,3]. Thus, managing knowledge requires determining which premises are true and false within a logical system. Another crucial function is guiding premises toward a desired conclusion, which is conceptually similar to goal-conditioned reinforcement learning [4].  For instance, we can pose a query:\n\n  “How do we make ( P = T ) within the premises if ( P = F ) initially?”\n\n\nThis question asks what assumptions ( \\Delta ) must be introduced to achieve a desired premises via non-monotonic reasoning [6]. One might also seek the minimal set ( \\Delta ) that efficiently transitions the premises. A key advantage of logical systems is that they provide a structured and consistent framework for knowledge representation [6]. Within this system, correctness can be verified without additional resources, ensuring that knowledge remains well-organized. In contrast, unstructured systems—where premises are added without clear rules—fail to guarantee consistency in truth values. A structured logical system ensures incremental and safe inference generation [7]. However, as the number of premises grows, efficiently evaluating consistency becomes increasingly difficult—this is known as the Frame Problem. Solving this requires more structured and optimized logical systems [8,9].\n\nConclusion\nHandling truth values in the real world is a fundamental challenge in formal reasoning, error minimization, and assumption-based inference. A logical system provides a framework for managing truth-value-based knowledge, ensuring that inference rules guide reasoning correctly. Moreover, logical frameworks allow verification of complex reasoning steps, even for advanced inference models such as default logic. By structuring reasoning step by step, logical systems ensure correctness and provide explainability in reasoning, making inference processes transparent and interpretable.\n\nDo LLMs Have Reasoning Ability?\n\n1. Do LLMs Have Knowledge Premises?\n\nA: Somehow, but No.\n\nLLMs possess implicit knowledge encoded within their vast parameters. This knowledge is stored in a way similar to key-value memories, where word embeddings capture relationships between concepts. However, unlike formal logical premises, this knowledge is distributed and statistically inferred rather than explicitly structured.\n\n2. Do LLMs Have Inference Rules?\n\nA: Clearly No.\n\nLLMs do not inherently follow explicit inference rules as formal logical systems do. While some attention heads might capture patterns that resemble certain inference mechanisms, these are not explicitly defined nor sufficiently expressive to guarantee consistent logical inference. Their reasoning is emergent and heuristic, rather than rule-based.\n\n3. Can LLMs Perform Long-Chain Reasoning?\n\nA: Clearly No.\n\nIn a structured logical system, a long chain of reasoning requires consistency of premises, correctness of inference steps, and logical soundness. LLMs, however, cannot guarantee consistency in their conclusions, nor can they verify whether their reasoning process adheres to formal logical constraints. They often exhibit hallucinations and spurious correlations, which are signs of pattern-based generalization rather than rigorous reasoning.\n\n4. Could LLMs Contain Embedded Reasoning?\n\nA: Almost No.\n\nOne possible way to argue that LLMs possess reasoning ability is by conjecturing:\n“All knowledge and inference rules are implicitly encoded within the model’s parameters, and they could be extracted when needed.”\n\nHowever, I find this impractical and infeasible. Even if inference rules were embedded in parameters, extracting them is not a formalized process because LLMs are highly sensitive to prompts and do not consistently retrieve information in a structured, rule-based manner.\n\n5. Can We Inject Knowledge and Inference Rules via Distribution Matching?\n\nA: Almost No.\n\nA natural question arises:\n“Can we inject explicit knowledge and inference rules into LLMs by aligning their distributions with a structured logical system?”\n\nWhile this idea is theoretically interesting, it is an extremely difficult problem in distributed representation systems. The main challenge is that inference rules in formal logic are discrete and symbolic, whereas the internal representations of LLMs are continuous and high-dimensional.\n\n\n  Mapping discrete rules into continuous vector spaces introduces a fundamental gap, as formal logic requires exact reasoning steps, while neural networks operate via soft constraints and approximations.\n  Compositionality in reasoning is also difficult to enforce in LLMs, since they rely on pattern recognition rather than rule-based inference. Even if a model can approximate certain reasoning behaviors, ensuring systematic generalization across logical structures remains an open challenge.\n\n\nConclusion: Do LLMs Truly Reason?\n\nA: Almost No.\n\nLLMs do not possess true reasoning ability in the formal logical sense. Instead, they operate as probabilistic pattern recognizers, relying on vast amounts of memorized relational knowledge and learned statistical correlations. While they can generate responses that mimic reasoning, they lack the ability to verify, correct, or consistently derive conclusions through explicit logical steps.\n\nEven if we consider injecting structured knowledge through distribution matching, the symbolic-to-continuous gap in representation makes it an impractical and infeasible solution. In essence, LLMs’ “reasoning” is an emergent phenomenon of large-scale pattern recognition, rather than a structured logical process.\n\nAppendix\n\n\n  \n    \n      Logic Type\n      Description &amp; Purpose\n      Key Reference\n    \n  \n  \n    \n      Default Logic\n      Allows reasoning with default assumptions when explicit knowledge is missing. Common in AI and expert systems.\n      Reiter (1980)\n    \n    \n      Default Reasoning (Poole)\n      A probabilistic extension of default logic that incorporates uncertainty in assumptions.\n      Poole (1988)\n    \n    \n      Independent Choice Logic\n      Models independent decisions in a probabilistic framework, useful for decision-making and game theory.\n      Poole (1997)\n    \n    \n      Modal Logic\n      Introduces necessity (□) and possibility (◇) operators, allowing reasoning about possibility, necessity, and obligations.\n      Kripke (1963)\n    \n    \n      Epistemic Logic\n      A modal logic focused on knowledge representation in multi-agent systems.\n      Hintikka (1962)\n    \n    \n      Doxastic Logic\n      Similar to epistemic logic but focuses on reasoning about beliefs rather than knowledge.\n      Konolige (1986)\n    \n    \n      Temporal Logic\n      Extends logic to include time-based reasoning, used in software and hardware verification.\n      Prior (1967)\n    \n    \n      Fuzzy Logic\n      Allows truth values between 0 and 1, enabling reasoning under vagueness and imprecision.\n      Zadeh (1965)\n    \n    \n      Probabilistic Logic\n      Integrates probability theory with logic to handle uncertainty.\n      Nilsson (1986)\n    \n    \n      Bayesian Logic\n      Uses Bayesian networks to model probabilistic inference.\n      Pearl (1988)\n    \n    \n      Paraconsistent Logic\n      Handles contradictions without collapsing into triviality. Useful in AI and philosophy.\n      da Costa (1974)\n    \n    \n      Relevance Logic\n      Ensures logical relevance in implication, avoiding paradoxes of classical logic.\n      Anderson &amp; Belnap (1975)\n    \n    \n      Intuitionistic Logic\n      Rejects the law of the excluded middle, emphasizing constructive proof methods.\n      Heyting (1930)\n    \n    \n      Higher-Order Logic (HOL)\n      Extends first-order logic to allow quantification over predicates and functions.\n      Church (1940)\n    \n    \n      Multi-Valued Logic\n      Expands beyond binary logic with multiple truth values (e.g., ternary, Łukasiewicz logic).\n      Łukasiewicz (1920)\n    \n    \n      Many-Worlds Semantics\n      A framework for reasoning about multiple possible worlds, useful in AI and philosophy.\n      Lewis (1973)\n    \n    \n      Defeasible Logic\n      A non-monotonic logic where conclusions can be overridden by stronger evidence.\n      Nute (1994)\n    \n    \n      Argumentation Logic\n      Models structured argumentation and conflict resolution in reasoning systems.\n      Dung (1995)\n    \n    \n      Description Logic\n      The formal foundation for ontologies and the Semantic Web.\n      Baader et al. (2003)\n    \n    \n      Linear Logic\n      Models resource-sensitive reasoning, used in computational logic and type theory.\n      Girard (1987)\n    \n    \n      Hybrid Logic\n      Combines modal and first-order logic for enhanced expressiveness.\n      Blackburn (1993)\n    \n    \n      Causal Logic\n      Introduces causality into logical reasoning, important in AI and philosophy of science.\n      Pearl (2000)\n    \n  \n\n\nApplication Roles of Logic\n\nN/A - I will write this part after learning more about logical systems.\n",
      "img": "assets/img/logic_roles.png",
      "type": "article"
    },
  
    
    
    
    {
      "title": "Epistemology – William James",
      "url": "/essay/epistemology12-james/",
      "tags": ["AI"],
      "year": "2025",
      "content": "🧑‍🏫 William James\n\n\n  Year: 1842–1910\n  Publications:\n    \n      The Principles of Psychology (1890)\n      The Will to Believe (1897)\n      Pragmatism (1907)\n      A Pluralistic Universe (1909)\n    \n  \n  Contributions:\n    \n      Popularized Pragmatism\n      Introduced the concept of stream of consciousness\n      Helped establish psychology as a scientific discipline\n      Defended the will to believe in uncertain situations\n      Proposed a pluralistic ontology of reality\n    \n  \n\n\n\n\n🔧 Pragmatism\n\nWilliam James saw truth not as a fixed, eternal object but as something that works—an idea is true if it has practical value in our lived experience. Truth, for James, is not found but made through a process of verification in real life.\n\n\n  “Truth is what works.”\n– William James\n\n\nHis approach strongly resonates with how we evaluate AI systems today—not by internal mechanisms alone, but by how they function in the real world.\n\n\n\n🐿️ The Squirrel Story\n\nTo explain pragmatism, James shared a famous anecdote:\n\nA man tries to see a squirrel on a tree. As he walks around the tree, the squirrel also moves around the opposite side, so the man never gets a full view. His friends ask:\n\n“Did the man go around the squirrel or not?”\n\nJames replies:\n\n\n  “That depends on what you mean by ‘around.’”\n\n\nIf “around” means moving in all four directions (north, east, south, west), then yes. But if it means changing position relative to the squirrel’s front, side, and back, then no.\n\nThe key insight? Concepts gain clarity only when defined in practical, result-oriented terms—a hallmark of pragmatism.\n\n\n\n🌊 Stream of Consciousness\n\nJames described human consciousness not as a fixed structure but as a continuous flow. He coined the term “stream of consciousness” to describe this ever-changing, selective, personal mental activity.\n\nFour characteristics of consciousness:\n\n\n  Personal – It belongs only to the individual and cannot be directly shared.\n  Continuous – It flows without interruption.\n  Ever-changing – Thoughts never stay still; they transition endlessly.\n  Selective – We pay attention only to some of the many inputs we receive.\n\n\nTime and the Sense of Self\n\nJames argued that consciousness includes a temporal sense of self—a feeling of being “here now,” “having been,” and “becoming.” This idea later influenced phenomenologists like Husserl and Henri Bergson’s concept of durée (duration).\n\nLiterary Influence\n\n\n  Influenced writers: James Joyce, Virginia Woolf, William Faulkner\n  In literature, stream of consciousness became a narrative technique that portrays raw mental flow, full of emotions, images, and memories.\n  It highlights how meaning, identity, and memory emerge through fluid, lived experience—not through rigid cognitive structures.\n\n\n\n\n🤝 Relationship with Peirce\n\nAlthough Charles Sanders Peirce first introduced the term pragmatism, it was William James who made it famous.\n\n\n  Peirce: Founded pragmatism as a logic-based method of clarifying ideas.\n  James: Popularized pragmatism in psychology, religion, and philosophy.\n  Peirce later coined “pragmaticism” to distinguish his own view from James’s broader, more psychological version.\n\n\n\n\n🧠 Relevance to Modern AI\n\nJames’s ideas are deeply relevant to how we understand and evaluate AI today.\n\n\n  A structuralist view asks: What components make up an AI system?\n  A pragmatist view asks: What does the AI do in real life? Does it help us?\n\n\nAI is physically just circuits and code. But when we interact with it, it may comfort us, offer intelligent answers, and solve problems. From a pragmatic lens, that makes it function like a mind.\n\nEven if AI doesn’t have emotions or consciousness in the biological sense, its effects on us are real. That interaction creates meaning—just as James claimed about consciousness.\n\n\n\n📌 Summary\n\nWilliam James emphasized:\n\n\n  Truth as something that works in practice\n  Consciousness as a flow, not a fixed state\n  Meaning as something defined by concrete outcomes\n  Philosophy as a tool to resolve real-world problems\n\n\n\n\n💡 AI and James Today\n\nJames’s thought teaches us to focus less on the essence of AI and more on its impact.\n\nIn evaluating AI systems, we shouldn’t just ask, “Is it conscious?”\nInstead, we should ask, “Does it affect us like a conscious being would?”\n\nThat shift—from what something is to what it does—is at the heart of pragmatism. And it’s a powerful way to understand not just AI, but the knowledge-based society we live in today.\n",
      "img": "assets/img/modeling_love.jpeg",
      "type": "article"
    },
  
    
    
    
    {
      "title": "Towards Understanding Symbolic AI",
      "url": "/essay/symbolic_ai/",
      "tags": ["AI"],
      "year": "2025",
      "content": "Towards Symbolic AI\n\nWith the rapid advancement of artificial intelligence, there is a growing interest in symbolic reasoning. This resurgence is particularly significant because humans tend to learn extensively by understanding, identifying, and forming relationships between symbols. Symbolic AI aims to model such processes by examining the relations among symbols and using formal systems—such as logical or probabilistic inference—to discover new symbolic connections.\n\nHowever, a persistent challenge in symbolic AI lies in the complexity of integrating benchmarks and hypotheses that arise from the study of symbols into modern AI systems. Rather than being clearly defined and embedded, these symbolic components are often entangled within broader architectures. For instance, the grounding problem—how to ensure that symbols carry meaning—offers crucial insight into the semantics and relational structures of symbols, including metaphysical grounding. Analogical reasoning, another key area, involves mapping relationships between symbols metaphorically. This is exemplified by vector-based relational models, such as those found in WordNet or word embedding spaces. In such models, semantic relationships can be captured by vector arithmetic (e.g., the vector from king to queen can be applied to gorilla to yield a new conceptual direction): $P = P_{\\text{gorilla}} + V_{\\text{queen}}$. This illustrates how symbolic meaning can emerge through transformations in a vector space.\n\nThese approaches offer a path toward mapping abstract concepts to symbols. Prior AI systems have explored how internal vector representations may acquire meaning through learning processes (e.g., via toy models), contributing to a deeper understanding of how vectors encode and manipulate conceptual directions.\n\nIn contrast, humans have developed rich traditions of dealing with symbols, informed by centuries of philosophical, linguistic, and cognitive research. Symbolic AI can benefit from integrating these traditions, helping large-scale systems like LLMs to better handle symbolic reasoning. This, in turn, could lead to safer, more interpretable, and more efficient model architectures, improved algorithms, and more robust data representations.\n\nAccordingly, the goal of this post is to provide a comprehensive overview of symbolic AI systems, exploring their foundations and offering insights into how they might evolve to support future research and practical applications.\n\nTwo Branches of Symbolic AI\n\n\n\nSymbol Grounding (Perception to Symbol Mapping)\n\nHarnad (1990) famously posed the symbol grounding problem:\n“How can the meanings of words be grounded in anything other than other words?”\n\nWhat are differences between the symbol grounding problem and classification tasks? (See the footnote Symbol Grounding is about how symbols get their meaning, especially from the world and experience. Classification is about how systems assign labels to data, based on patterns in training data..)\n\nSymbolic logic systems must search over all possible instantiations of symbols, even when the rules are known.\n\n Algorithm Examples \n\n\n  \n    DeepProbLog (Manhaeve et al., 2018) [go below]: The input is processed by a neural predicate digit(Img, Digit), which uses a neural network to produce probabilistic outputs. The error is computed after symbolic reasoning via Prolog, and only the neural network parameters are trained.\n  \n  \n    Neuro-Symbolic Concept Learner (NS-CL, Mao, 2019) [go below]:\n  Given an image and a natural language question, the system first extracts object-level feature vectors from the image. These features are processed through various neural operators (e.g., ShapeOf(obj), ColorOf(obj)) to generate soft attribute estimates. In parallel, the natural language question is parsed into a structured program represented in a domain-specific language (DSL),\n  which defines a sequence of symbolic operations (e.g., Filter, Relate, Query).  The extracted soft facts—probabilistic evaluations of visual concepts—are then used as inputs to execute the DSL program. This execution is performed not by a traditional symbolic engine, but by a quasi-symbolic executor, also referred to as a neuro-symbolic program executor. This executor evaluates the symbolic program over soft neural outputs,\n  enabling differentiable reasoning and end-to-end learning.\n  \n  \n    NS-VQA (Neuro-Symbolic Visual QA, Yi et al., 2018) [go below]: Uses Mask R-CNN to extract visual features and converts natural language questions into a domain-specific language (DSL) using a GRU-based seq2tree model. The resulting logic program is executed as a symbolic reasoning process via a Python program.\n  \n  LOGIC-LM (Pan et al., 2023) [go below]\nA neuro-symbolic reasoning system that combines LLMs with symbolic solvers for faithful logical inference. Shift reasoning execution from LLM to symbolic solvers, leveraging LLMs only for translation (symbol grounding).\n    \n      Problem Formulator – LLM converts a natural language problem into a symbolic representation (FOL, LP, CSP, SAT).\n      Symbolic Reasoner – Deterministic symbolic solver (e.g., Prover9, Pyke, Z3) performs logical inference.\n      Result Interpreter – Maps symbolic result back to natural language.\n      Self-Refiner – Uses solver error messages to revise invalid symbolic forms via iterative prompting.\n    \n  \n  A-NESI (Krieken et al., 2023) [go below]\n(Approximate Neurosymbolic Inference)** is a scalable framework that combines neural networks with symbolic reasoning for probabilistic neurosymbolic learning tasks. Unlike traditional methods that rely on exact inference and suffer from exponential time complexity, A-NESI uses neural models to perform approximate inference in polynomial time. It separates prediction and explanation into two neural components trained on synthetic data generated from background knowledge. Additionally, it supports logical constraints at test time through a symbolic pruning mechanism, making it well-suited for safety-critical applications.\n\n\nInductive Logic Program (Rule Learning)\n\n\n  \n    Inductive Logic Programming, Muggleton, S. (1991)\n  \n  \n    FOIL: Learning logical definitions from relations, Quinlan, J. R. (1990)\n  \n  \n    FOCL:\n  \n  \n    Progol (Muggleton, 1995)\n  \n  \n    Metagol system for learning meta-interpreted programs, Cropper, A., &amp; Muggleton, S. (2016)\n  \n  \n    ∂ILP (Differentiable ILP) [go below]: ∂ILP is a differentiable Inductive Logic Programming system that learns symbolic rules from relational data through gradient-based optimization. It replaces discrete inference with differentiable conjunction and disjunction neurons operating over soft truth values. This design enables interpretable rule learning, supports recursion and predicate invention, and generalizes to unseen examples without relying on hand-crafted rule templates.\n  \n  \n    pLogicNet (not strict ILP because trains a weight for a rule) [go below]: pLogicNet is a probabilistic logic neural network that combines the strengths of symbolic logic reasoning and embedding-based knowledge graph completion. It uses predefined logic rules (e.g., composition, inverse, symmetry) and optimizes their weights using a Variational EM algorithm. While it does not learn new rules from scratch, it updates the influence of known rules based on both observed and inferred triples, bridging statistical learning with logical consistency.\n  \n  \n    Neural Theorem Provers, Rocktäschel &amp; Riedel, 2017\n  \n  \n    Inductive Logic Programming via Differentiable Forward Chaining, Payani &amp; Fekri (2019)\n  \n  \n    Differentiable Learning of Logical Rules for Knowledge Base Reasoning, Yang, Z. et al. (2017)\n  \n  \n    Logical neural networks, Campero, A. et al. (2018)\n  \n  \n    Learning explanatory rules from noisy data, Evans, R., &amp; Grefenstette, E. (2018)\n  \n  \n    Learning Big Logical Rules by Joining Small Rules, Hocquette, 2024\n  \n  \n    Neural Logic Machines, Dong, 2019\n  \n  \n    NeuPSL (Neural Probabilistic Soft Logic)\n  \n  \n    LTN (Logic Tensor Networks)\n  \n  \n    A-NESI : Approximate Neurosymbolic Inference\n  \n  \n    Scallop (Li, 2023) [go below]: Scallop is a neurosymbolic programming language that integrates deep learning with symbolic reasoning through a differentiable logic framework. It allows users to define logical rules in a Datalog-inspired language and combine them with neural models for end-to-end learning. While users provide templates or rule structures, Scallop learns how to map those to task-specific predicates using training data. Predicate names such as parent or ancestor are typically defined in advance, and the system searches for the best combination of these predicates to satisfy a target objective. This makes Scallop suitable for tasks requiring both perceptual grounding and logical generalization, including knowledge reasoning, planning, and multimodal learning.\n  \n  \n    NeSyA: Neurosymbolic Automata\n  \n\n\n\n\n🧠 ALGORITHM: DeepProbLog\n\nMahaeve proposed Probabilistic Logic Programming (DeepProbLog) in 2018. The algorithm trains a neural predicate which is defined by the following format.\nnn(m, InputArgs, OutputVar, OutputDomain) :: Predicate.\n\n\nFor example, the digit predicate for an image and symbol digit could be defined by:\n\nnn(mnist_net, [Img], Digit, [0,1,2,3,4,5,6,7,8,9]) :: digit(Img, Digit).\n\n\nnn(digit_net, [Img], Digit, [0..9]) :: digit(Img, Digit).\nnn(op_net, [Img], Op, [+,-,*,/]) :: operator(Img, Op).\n\nsolve(E1, E2, E3, Result) :-\n    digit(E1, D1),\n    digit(E3, D2),\n    operator(E2, Op),\n    eval(Op, D1, D2, Result).\n\neval(+, A, B, R) :- R is A + B.\neval(-, A, B, R) :- R is A - B.\n\n\n\n\n🧠 ALGORITHM: NS-CL\n\nMao proposed Neuro-Symbolic Concept Learner in 2019.\n\n  The natural language question is mapped to a structured symbolic program.\n  Execution is performed through differentiable neural operators like ColorOf() and PositionOf().\n  The result is a probabilistic symbolic reasoning trace that is fully trainable end-to-end.\n\n\nConsider a color vectors:\n# Assume 3 color concepts: Red, Blue, Green\nv_red   = torch.tensor([0.9, 0.1, 0.0])\nv_blue  = torch.tensor([0.2, 0.8, 0.0])\nv_green = torch.tensor([0.1, 0.2, 0.9])\nconcepts = torch.stack([v_red, v_blue, v_green])  # (3, d)\n\n\n# Object feature (from ResNet)\nf_obj = torch.tensor([0.85, 0.15, 0.1])  # example object feature\n\n# Predict color distribution\ncolor_probs = ColorOf(f_obj, concepts)\nprint(color_probs)  # e.g., tensor([0.81, 0.15, 0.04])\n\n\nWe have a question in the form of natural language: \n“What is the color of the right object?” It is converted into a DSL form:\n# DSL Program:\nProgram = Query(Color, Filter(Rightmost))\n\n\n# Step 2: Apply Filter(Rightmost) - select the rightmost object\nright_scores = [PositionOf(obj).x for obj in object_features]  # Get x-coordinate\nrightmost_index = argmax(right_scores)                        # Index of rightmost object\nmask = one_hot(len(object_features), rightmost_index)         # Binary mask for that object\n\n\n# Step 3: Apply Query(Color) - predict the color of the selected object\nselected_feat = weighted_sum(object_features, mask)       # Soft selection\ncolor_probs = ColorOf(selected_feat, color_concepts)      # Probability over color concepts\n\n# Final Answer:\npredicted_color = argmax(color_probs)  # e.g., \"Red\"\n\n\n\n  Question: Why this algorithm is called Neuro-Symbolic?   From the visual scene, the algorithm extracts the probability of each symbolic concept, and the natural language question is transformed into a domain-specific language (DSL). This program directly evaluates symbolic conditions, although the evaluation itself is probabilistic.   \n\n\n\n\n🧠 ALGORITHM: LOGIC-LM\n\nPan et al. introduced LOGIC-LM in 2023.  A neuro-symbolic framework that decouples reasoning from language generation by having LLMs generate symbolic representations, and symbolic solvers execute logical inference.\nLOGIC-LM delegates:\n\n  Language understanding → LLM\n  Symbolic inference → External solver\n  Ensures logical faithfulness, robustness, and interpretability\n\n\nInput:\n\n\n  Natural language problem (e.g., multiple-choice or free-form question)\n\n\n\"Stranger Things\" is a popular Netflix show.\nIf a Netflix show is popular, Karen will binge-watch it.\nIf and only if Karen binge-watches a Netflix show, she will download it.\nKaren does not download \"Black Mirror\".\n\"Black Mirror\" is a Netflix show.\nIf Karen binge-watches a Netflix show, she will share it to Lisa.\n\nQuestion: Is the following statement true, false, or uncertain?  \n\"Black Mirror\" is popular. (A) True  (B) False  (C) Uncertain\n\n\nProblem Formulator (LLM-generated symbolic form):\n\nPredicates:\nNetflixShow(x)        # x is a Netflix show\nPopular(x)            # x is popular\nBingeWatch(x, y)      # x binge-watches y\nDownload(x, y)        # x downloads y\nShare(x, y, z)        # x shares y to z\n\nFacts:\nNetflixShow(strangerThings) ∧ Popular(strangerThings)\n∀x (NetflixShow(x) ∧ Popular(x) → BingeWatch(karen, x))\n∀x (NetflixShow(x) ∧ BingeWatch(karen, x) ↔ Download(karen, x))\nNetflixShow(blackMirror) ∧ ¬Download(karen, blackMirror)\n∀x (NetflixShow(x) ∧ BingeWatch(karen, x) → Share(karen, x, lisa))\n\nQuery:\nPopular(blackMirror)\n\n\nSymbolic Reasoner Output:\n\nResult: false\n\n\nResult Interpreter Output:\n\nAnswer: (B) False\n\n\nSelf-Refiner (if symbolic execution fails):\n\n\n  Receives error from symbolic solver (e.g., “unbound variable”)\n  LLM revises symbolic form using in-context error correction examples\n  Retries execution until success or max attempts\n\n\nWhich Symbolic Engine the LLMs use?\n\nAn LLM gets a prompt describing the a dedicated task.\n\n\n  Deductive reasoning → Logic Programming (LP) → Pyke\n  First-order logic → FOL → Prover9\n  Constraint satisfaction → CSP → python-constraint\n  Analytical reasoning → SAT → Z3\n\n\n\n\n🧠 ALGORITHM: A-NESI\n\n🔍 Symbolic Prediction vs Neural Prediction in A-NESI\nA-NESI is a scalable framework for Probabilistic Neurosymbolic Learning (PNL) that combines neural perception with symbolic reasoning — without relying on expensive exact inference.\n\n  ✅ Scalable Approximate Inference in polynomial time\n  🧠 Neural models for both prediction and explanation\n  📘 Symbolic reasoning remains intact (no semantic loss)\n  💬 Explainability via most probable world inference\n  🔐 Constraint satisfaction using symbolic pruning\n  🔄 Trained using data generated from background knowledge\n\n\n🧩 Core Components\n\nGiven an input \\(x\\) (e.g., images of digits), the perception model \\(f(x)\\) outputs a belief:\n\n\\[P = f(x)\\]\n\nwhere \\(P\\) is a distribution over possible symbolic worlds \\(w\\) (e.g., digit pairs like (5,8)).\n\nThe symbolic reasoning function \\(c(w)\\) computes the deterministic output from a world:\n\n\\[y = c(w)\\]\n\nThis captures prior knowledge such as digit summation or Sudoku validity rules.\n\nA-NESI uses a joint factorization of the output distribution:\n\n\\[q(w, y \\mid P) = q(y \\mid P) \\cdot q(w \\mid y, P)\\]\n\nHere, the prediction model \\(q(y \\mid P)\\) generates the output autoregressively, while the explanation model \\(q(w \\mid y, P)\\) identifies the most likely symbolic world that explains the prediction.\n\nTo train the system, a belief prior \\(p(P)\\) is used to generate synthetic training data. The symbolic function \\(c(w)\\) is applied to each sampled world to produce the supervised output \\(y = c(w)\\). The prediction model is trained by minimizing the following loss:\n\n\\[\\mathcal{L}_{\\text{Pred}} = \\mathbb{E}_{(P, w)} \\left[ -\\log q(c(w) \\mid P) \\right]\\]\n\nAdditionally, the explanation model can be trained using a joint matching loss to align the predicted and true joint distributions:\n\n\\[\\mathcal{L}_{\\text{Expl}} = \\mathbb{E}_{(P, w)} \\left[ \\left( \\log q(w, c(w) \\mid P) - \\log p(w \\mid P) \\right)^2 \\right]\\]\n\n\n  \n    \n      Aspect\n      🧾 Symbolic Prediction\n      🧠 Neural Prediction\n    \n  \n  \n    \n      Input\n      \\(P = f(x)\\)\n      \\(P = f(x)\\)\n    \n    \n      Output generation\n      \\(w = \\arg\\max_w \\, p(w \\mid P), \\quad y = c(w)\\)\n      \\(q(y \\mid P) = \\prod_{i=1}^{k_Y} q(y_i \\mid y_{&lt;i}, P)\\)\n    \n    \n      Reasoning function\n      Uses symbolic reasoning \\(c(w)\\)\n      No symbolic function; reasoning is learned implicitly\n    \n    \n      Architecture\n      Sampling + symbolic function\n      RNN-style or Transformer-style autoregressive decoder\n    \n    \n      Interpretability\n      ✅ High: prediction traceable through \\(w\\) and \\(c(w)\\)\n      ❌ Low: no explicit reasoning path\n    \n    \n      Constraint satisfaction\n      ✅ Yes, via symbolic constraints \\(c(w)\\)\n      ❌ Not guaranteed (unless symbolic pruning is applied)\n    \n    \n      Inference speed\n      🐢 Slower (but scalable with symbolic pruning)\n      ⚡ Fast and parallelizable on GPU\n    \n    \n      Accuracy on large \\(N\\)\n      ✅ Stable even for \\(N = 15\\)\n      ⚠ May degrade at large \\(N\\) (e.g., MNISTAdd with \\(N = 15\\))\n    \n    \n      Training role\n      Validates predictions\n      Trains \\(f(x)\\) using gradients through \\(q(y \\mid P)\\)\n    \n    \n      Best suited for\n      Safety-critical, explainable AI\n      Fast inference and large-scale applications\n    \n  \n\n\nSymbolic pruning in A-NESI improves inference efficiency and ensures logical correctness by eliminating invalid options during the step-by-step generation of symbolic variables. As the model generates each variable (e.g., \\(w_i\\)), a task-specific pruning function \\(s_{y, w_{1:i-1}}(w_i)\\) is applied to mask values that violate constraints defined by the symbolic function \\(c(w)\\). This pruning results in a modified distribution:\n\n\\[q'(w_i \\mid w_{1:i-1}, y, P) \\propto q(w_i \\mid \\cdot) \\cdot s_{y, w_{1:i-1}}(w_i)\\]\n\nfollowed by renormalization:\n\n\\[q'(w_i = j \\mid \\cdot) = \\frac{q(w_i = j \\mid \\cdot) \\cdot s(j)}{\\sum_{j'} q(w_i = j' \\mid \\cdot) \\cdot s(j')}\\]\n\nFor example, in MNISTAdd with target sum \\(y = 13\\), if \\(w_1 = 9\\), only \\(w_2 = 4\\) is valid since \\(9 + 4 = 13\\). All other values are pruned using:\n\n\\[s_{y, w_1}(j) =\n\\begin{cases}\n1 &amp; \\text{if } w_1 + j = y \\\\\n0 &amp; \\text{otherwise}\n\\end{cases}\\]\n\nSymbolic pruning is especially important in structured tasks like Sudoku or path planning, and the pruning function must be defined per task using logical rules or constraint checkers.\n\n\n\n🧠 ALGORITHM: NS-VQA\n\nNS-VQA (Neuro-Symbolic Visual QA) – Detailed Explanation\n\n\n  Step 1: Scene Parsing (Visual Understanding)\n\n\nThe process begins with an input image that contains various objects. These objects are segmented using Mask R-CNN, which detects and outlines each object in the scene. Once the objects are identified, a convolutional neural network (CNN) processes these segments to extract detailed features such as shape, size, material, color, and 3D position coordinates (x, y, z). These features are organized into a structured scene representation table, where each row corresponds to one object and lists its attributes.\n\n\n  Step 2: Question Parsing (Program Generation)\n\n\nNext, the system takes a natural language question as input—such as “How many cubes that are behind the cylinder are large?”—and converts it into a symbolic program. This conversion is performed by a GRU-based LSTM model (a type of seq2tree architecture). The model generates a series of logical operations in a domain-specific language (DSL), forming a symbolic program. For the example question, the generated steps might include filtering for cylinders, identifying objects behind them, filtering those objects for cubes, narrowing down to large ones, and finally counting them.\n\n\n  Step 3: Program Execution (Reasoning)\n\n\nThe symbolic program is then executed using a Python-based symbolic executor. This executor operates on the structured scene representation to perform reasoning tasks like filtering, spatial relation extraction, and attribute comparison. Each operation manipulates the data step by step, narrowing it down based on the program logic. In the example, the system would end up with a set of large cubes behind the cylinder and return the count—say, 3—as the final answer.\n\n\n  Performance Summary\n\n\nNS-VQA achieves remarkably high accuracy on the CLEVR dataset, outperforming most existing methods. When trained with 270 symbolic programs, it achieves 99.8% overall accuracy. It performs especially well in logically intensive tasks such as counting, comparison, and attribute querying, showing that combining neural perception with symbolic reasoning leads to powerful and interpretable AI systems.\n\n\n\n🧠 ALGORITHM: Differentiable ILP\n\nOverview\nDifferentiable ILP (∂ILP) is a neural-symbolic model that learns logical rules from data through differentiable forward chaining. It replaces discrete logical inference with neural computation and enables end-to-end learning without hand-designed rule templates.\n\nCore Components\n\n  \n    Ground Atom Valuations:\nEach fact (e.g., father(alice, bob)) is assigned a continuous truth value ∈ [0, 1], representing its current belief level. These soft valuations serve as the model’s internal working memory.\n  \n  Logical Neurons:\n    \n      Conjunction Neuron (fuzzy AND):\nOutput = product of selected input truth values.\n      Disjunction Neuron (fuzzy OR):\nOutput = 1 - product of complements (i.e., fuzzy OR).\nEach neuron has trainable weights (via sigmoid activations) that determine which atoms participate in the logical clause.\n    \n  \n  \n    Clause Composition:\nThe neurons form a layered structure approximating a DNF or CNF formula. Rules are represented as differentiable logic programs where each clause is a soft conjunction or disjunction of atoms.\n  \n  \n    Forward Chaining (Iterative Reasoning):\nInference is performed iteratively: at each step, the model updates the truth values of atoms using the current rules. This simulates how new facts are derived over time.\n  \n  \n    Loss and Training:\nThe model is trained by minimizing the cross-entropy between predicted truth values and ground-truth labels. Gradients propagate through the entire reasoning process, enabling the discovery of rule structure and content.\n  \n  Predicate Invention and Recursion:\nIntermediate atoms (auxiliary predicates) can be created and reused across steps, enabling recursive definitions and higher expressivity in learned logic programs.\n\n\nAdvantages\n\n  Learns interpretable symbolic rules with neural gradients\n  Avoids reliance on rule templates or expert priors\n  Supports recursion and predicate invention\n  Bridges symbolic reasoning and differentiable optimization\n\n\n\n\n🧠 ALGORITHM: Scallop\n\nScallop is a neurosymbolic programming language that bridges neural perception and symbolic reasoning through differentiable logic programming. It allows users to define logical rules in a declarative language similar to Datalog and integrate them with neural network models in an end-to-end learnable system. The central idea is to separate perception and reasoning: a neural model processes raw input (such as an image or text) into intermediate symbolic representations, and a logic program applies rules over those representations to produce the final output.\n\nA key feature of Scallop is that while the structure of rules can be given in the form of templates—such as Q(X, Y) :- R(X, Z), S(Z, Y)—the actual mapping of these variables to task-specific predicates (e.g., Q = ancestor, R = parent, S = ancestor) is learned from data. This enables the system to generalize over symbolic patterns without requiring full supervision on internal structures. In most applications, the base predicates like parent, friend, or colleague are defined in advance, and Scallop searches over combinations of those to learn rules that best explain the output.\n\nFor example, in a knowledge reasoning task, the model may be asked to infer the ancestor(X, Y) relation. Given known facts like parent(A, B) and parent(B, C), Scallop can learn to compose these into recursive rules that define ancestry. The learning process optimizes both the parameters of the neural perception module and the symbolic reasoning path using a framework based on provenance semirings, allowing gradients to flow from output supervision back through symbolic programs and into the neural components.\n\nScallop supports recursion, negation, and aggregation in its logic programs, and can be used across a range of domains including visual reasoning, program induction, planning, and reinforcement learning. By combining structured reasoning with perceptual learning in a differentiable and modular way, Scallop enables both interpretability and scalability in neurosymbolic systems.\n\n📦 Example: Scallop Code\n\n// Knowledge base facts\nrel is_a(\"giraffe\", \"mammal\")\nrel is_a(\"tiger\", \"mammal\")\nrel is_a(\"mammal\", \"animal\")\n\n// Knowledge base rule\nrel name(a, b) :- name(a, c), is_a(c, b)\n\n// Recognized from an image (neural model output)\nrel name = {\n  0.3::(1, \"giraffe\"),\n  0.7::(1, \"tiger\"),\n  0.9::(2, \"giraffe\"),\n  0.1::(2, \"tiger\"),\n}\n\n// Aggregation query\nrel num_animals(n) :- n = count(o: name(o, \"animal\"))\n\n\n🔍 What is Given vs. What is Trained\n\n\n  \n    \n      Component\n      Given (Static) ✅\n      Trained (Learned) 🧠\n    \n  \n  \n    \n      Facts (e.g., is_a(\"tiger\", \"mammal\"))\n      ✅ Provided explicitly in logic\n       \n    \n    \n      Rule templates (e.g., Q(X,Y) :- R(X,Z))\n      ✅ Given as abstract logical structure\n       \n    \n    \n      Predicate vocabulary (e.g., is_a, name)\n      ✅ Declared in program or data schema\n       \n    \n    \n      Neural predictions (e.g., name = {...})\n      ❌ Produced by trained neural model\n      ✅ Neural model learns from input data\n    \n    \n      Rule-body mappings (e.g., Q = ancestor)\n      🔄 Can be fixed or learned (ILP-style)\n      ✅ Selected based on performance from data\n    \n    \n      Final prediction (e.g., num_animals(n))\n      ❌ Derived via symbolic reasoning\n      ✅ Supervised through end-to-end training\n    \n  \n\n\n\n\n🧠 ALGORITHM: pLogicNet\n\npLogicNet merges symbolic logic (e.g., Markov Logic Networks) with embedding-based models (e.g., TransE, DistMult).\n\n  It uses predefined logic rules such as Composition, Inverse, Symmetric, and Subrelation.\n  These rules are not learned but their weights are optimized using a Variational EM algorithm.\n  \n    The model enhances knowledge graph reasoning by combining neural predictions with symbolic consistency.\n  \n  ✅ Does not induce new rules; instead, it updates rule weights.\n  ✅ Bridges embedding-based KGE and logical consistency.\n  ✅ Resembles Datalog in how it applies symbolic rules.\n  ✅ Enables interpretable reasoning while preserving neural scalability.\n\n\nLearning Procedure (Variational EM)\n\n1. E-Step (Expectation)\n\n  Use a KGE (Knowledge Graph Embedding) model to infer hidden triples.\n  Apply predefined logical rules to expand the inferred graph (via the Markov Blanket).\n\n\n2. M-Step (Maximization)\n\n  Update the weights of logical rules using observed and inferred triples.\n  Optimize the pseudo-likelihood function for probabilistic inference.\n\n\nExample\n\n(A) Newton — BornIn — UK  \n(B) UK — LocatedIn — Europe\n\nUsing a composition rule, infer:  \n→ Newton — LocatedIn — Europe\n\nFinal Score = 0.82 (KGE) + λ × 1.0 (logical rule inference)\n\n\n\n\nReferences\n\n\n  \n    Mahaeve, DeepProbLog: Neural Probabilistic Logic Programming, 2018\n  \n  \n    Mao, The Neuro-Symbolic Concept Learner: Interpreting Scenes, Words, and Sentences From Natural Supervision, 2019\n  \n  \n    PAN, LOGIC-LM: Empowering Large Language Models with Symbolic Solvers for Faithful Logical Reasoning, 2023\n  \n  \n    Li, Scallop: A Language for Neurosymbolic Programming, 2023\n  \n  \n    Qu et al, Probabilistic Logic Networks for Reasoning, 2019\n  \n\n",
      "img": "",
      "type": "article"
    },
  
    
    
    
    {
      "title": "From Tokens to Spaces: Rethinking Semantics Across Language and Vision",
      "url": "/essay/vision_and_language/",
      "tags": ["AI"],
      "year": "2025",
      "content": "Comparison between Visual and Linguistic Features\n\nVisual models inevitably contain conceptual structures within their sparse autoencoder (SAE) representations, but interpreting these concepts seems far less straightforward than in language. With language, semantics tend to be immediately determined by the tokens themselves, whereas visual semantics resist such direct mapping. Visual distinctions may allow for clustering and alignment, but the function that links representations to meaning is harder to establish than in linguistic domains.\n\nIn particular, linguistic representations can often be chained together in a chain-of-thought style, linking features to form logical circuits. Visual representations, however, emphasize continuity and spatial relations more than explicit logic. Perhaps following the trajectory of visual forms resembles the human mode of imaginative thinking, where spatial reasoning supports higher cognition. In this sense, visual encodings could be essential tools for helping humans reason about spatial and dynamic structures.\n\nIf language encodings capture relationships between explicit symbols and their conceptual anchors, visual encodings may capture concepts of space itself and the objects inhabiting it. To achieve this, we must deepen our understanding of the causal dynamics that unfold in space and time. Using language is like building on strong anchor points that directly evoke concepts, while using visual representations requires interpreting ambiguous shapes, their movements, transformations, and relations.\n\nUltimately, models will move toward integrating images and texts, just as humans combine logical information with spatiotemporal understanding. The challenge lies in how such vast streams of information can be efficiently encoded, stored, and interrelated. I firmly believe that AI’s modes of thought will elevate human cognition. Humans learn continuously from birth through imitation, and to support that process, we need ever more advanced models to imitate. Lessons and wisdom take shape in surprising forms that move us. I am only searching for those yet-unrevealed forms of wonder.\n\n\n\nAdditional Ideas\n\nThe difficulty of interpreting visual encodings stems from their continuous, compositional, and dynamic nature. Unlike discrete tokens in language, visual inputs vary smoothly under rotation, scale, and illumination. The same object may appear in countless forms, making direct semantic assignment elusive. Moreover, visual scenes are rarely composed of isolated elements—they blend objects, relations, and backgrounds in ways that resist clean symbolic partitioning. This makes the semantic “anchors” of vision inherently less stable than those of language.\n\nOne promising direction is object-centric learning, where models attempt to decompose a scene into object slots that serve as candidates for concepts. Each slot can then be aligned with linguistic tokens or grounded in functional predictions. Yet static object recognition is not enough: many visual semantics emerge from dynamics and causality. Predicting how balls collide, how liquids flow, or how agents interact in space teaches models not only what objects are but also how they relate and influence each other. Such causal grounding is indispensable for deeper semantic understanding.\n\nLanguage and vision also differ in their compositional structures. In language, chain-of-thought reasoning can be viewed as a form of symbolic circuitry, where features activate along logical pathways. Vision, by contrast, seems more like continuity of concepts in a manifold: shapes, colors, and trajectories flow into one another, producing meaning not through explicit logic but through gradual transformations. This makes visual reasoning closer to imagination, where humans intuitively “see” possibilities in space rather than deduce them step by step.\n\nFrom a methodological perspective, models that combine contrastive learning (e.g., CLIP) with scene graphs or graph neural networks can begin to bridge this gap. Contrastive objectives align ambiguous visual slots with strong linguistic anchors, while graph-structured reasoning explicitly encodes relationships among objects. At the same time, causal interventions—such as altering colors, positions, or dynamics in synthetic data—provide a way to test whether visual concepts correspond to meaningful features.\n\nThe larger trajectory is toward multimodal cognition, where models learn to integrate symbolic and spatial representations. Language provides discrete, explicit handles on meaning; vision provides continuity, embodiment, and dynamic grounding. Together, they approximate the full richness of human cognition, which relies on both logic and imagination. If these representational strategies are properly unified, AI may not only mimic but also augment human reasoning, offering us new conceptual anchors, new ways to imagine, and new forms of wisdom yet unseen.\n",
      "img": "",
      "type": "article"
    },
  
    
    
    
    {
      "title": "Roles of Logical System",
      "url": "/essay/neurips2025/",
      "tags": ["AI"],
      "year": "2025",
      "content": "\n",
      "img": "assets/img/logic_roles.png",
      "type": "article"
    },
  
    
    
    
    {
      "title": "NeurIPS2025 Reading List",
      "url": "/essay/reading_list_neurips2025/",
      "tags": ["Research Life"],
      "year": "2025",
      "content": "\n  \n    \n      Paper Title\n      Note\n    \n  \n  \n    \n      A Implies B: Circuit Analysis in LLMs for Propositional Logical Reasoning\n      Handles explicit logical reasoning, internal circuit analysis clarifying the types of logics for internal interpretation\n    \n    \n      Multi-head Transformers Provably Learn Symbolic Multi-step Reasoning via Gradient Descent\n      Theoretical guarantee that Transformers can learn multi-step symbolic reasoning\n    \n    \n      Transformers Provably Learn Chain-of-Thought Reasoning with Length Generalization\n      Formal results: CoT reasoning provably learned, with length generalization\n    \n    \n      Reasoning by Superposition: A Theoretical Perspective on Chain of Continuous Thought\n      Frames continuous thought as superposition, theoretical perspective\n    \n    \n      LogicTree: Improving Complex Reasoning of LLMs via Instantiated Multi-step Synthetic Logical Data\n      Multi-step synthetic logical data to improve complex reasoning\n    \n    \n      SynLogic: Synthesizing Verifiable Reasoning Data at Scale\n      Large-scale verifiable logical data synthesis, useful for logical generalization\n    \n    \n      Enigmata: Scaling Logical Reasoning in LLMs with Synthetic Verifiable Puzzles\n      Uses logical puzzles for scalable, verifiable reasoning\n    \n    \n      Compositional Neural Network Verification via Assume-Guarantee Reasoning\n      Assume-Guarantee Reasoning (AGR) for modular verification, aligns with argument structure\n    \n    \n      VeriThoughts: Formal Verification Pipeline\n      Combines formal verification with code generation &amp; reasoning\n    \n    \n      Evaluating Program Semantics Reasoning with Type Inference in System F\n      Evaluates program semantics reasoning using type theory\n    \n    \n      Reviving DSP for Advanced Theorem Proving\n      Advanced theorem proving (ATP) via DSP techniques\n    \n    \n      IneqSearch / Ineq-Comp\n      Inequality theorem proving, suitable for argument/strategy structure experiments\n    \n    \n      On Learning Verifiers for Chain-of-Thought Reasoning\n      Studies how to learn verifiers for CoT reasoning loops\n    \n    \n      Right for the Right Reasons: Avoiding Reasoning Shortcuts via Prototype-Augmented Neurosymbolic AI\n      Focus on neurosymbolic reasoning + bias/shortcut avoidance, ensures faithfulness\n    \n    \n      Grammars of Formal Uncertainty: When to Trust LLMs in Automated Reasoning Tasks\n      Introduces formal grammars of uncertainty for trust calibration\n    \n    \n      A Theoretical Study on Bridging Internal Probability and Self-Consistency for LLM Reasoning\n      Theoretical link between probability and self-consistency for reasoning\n    \n    \n      SATURN: SAT-based Reinforcement Learning to Unleash Language Model Reasoning\n      Combines SAT solvers with RL to improve reasoning efficiency\n    \n    \n      Counterfactual reasoning: an analysis of in-context emergence\n      Studies counterfactual reasoning and in-context dynamics\n    \n    \n      Mathematical Reasoning Planning for Language Models\n      Planning framework for structured mathematical reasoning\n    \n    \n      DuetGraph: Coarse-to-Fine Knowledge Graph Reasoning\n      KG reasoning pipeline, coarse-to-fine\n    \n    \n      K-DeCore: Continual Structured Knowledge Reasoning\n      Continual learning for structured knowledge reasoning\n    \n    \n      GRIP: A Graph-Based Reasoning Instruction Producer\n      Produces reasoning instructions via graphs\n    \n    \n      Deliberation on Priors: Trustworthy Reasoning of LLMs on Knowledge Graphs\n      Explores trustworthy KG reasoning using priors\n    \n    \n      Personalized Decision Modeling: Utility Optimization or Textualized-Symbolic Reasoning\n      Decision-making models with symbolic reasoning\n    \n    \n      SymRTLO: Neuron-Inspired Symbolic Reasoning\n      Neuron-inspired symbolic reasoning for RTL optimization\n    \n    \n      Composing Global Solutions via Algebraic Objects in Neural Nets\n      Uses algebraic objects for compositional global reasoning\n    \n    \n      Multimodal Symbolic Logical Reasoning\n      Extends symbolic + logical reasoning into multimodality\n    \n    \n      What’s in Common? Multimodal Models Hallucinate When Reasoning Across Scenes\n      MLLM hallucinations in cross-scene reasoning\n    \n    \n      When Thinking Drifts: Evidential Grounding for Robust Video Reasoning\n      Addresses drift in multimodal reasoning, grounded in evidence\n    \n    \n      Collective Reasoning in Performative Prediction\n      Studies collective reasoning phenomena\n    \n    \n      Scientists’ First Exam: Probing Cognitive Abilities of MLLM\n      Probes MLLM cognitive abilities (perception, reasoning, understanding)\n    \n    \n      Can MLLMs Absorb Math Reasoning Abilities from LLMs as Free Lunch?\n      Evaluates transfer of math reasoning from LLMs to MLLMs\n    \n    \n      Mechanistic Interpretability of RNNs emulating Hidden Markov Models\n      Mechanistic interpretability applied to RNN–HMM dynamics\n    \n    \n      The Non-Linear Representation Dilemma: Is Causal Abstraction Enough for Mechanistic Interpretability?\n      Questions limits of causal abstraction in mechanistic interpretability\n    \n  \n\n",
      "img": "",
      "type": "article"
    }
  
] 