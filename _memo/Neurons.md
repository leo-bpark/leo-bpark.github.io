---
layout: default
title: 'Neurons'
img: /assets/img/logos/neurons.png
description: Google 
gradient: linear-gradient(135deg, #0064e1 0%, #5bd3ff 100%)
hover-gradient: linear-gradient(135deg, #00c6fb 0%, #005bea 100%)
date: 2021-01-21

---


| Tag | Title | Year |
|---| -----| ----| 
| ACL  | Knowledge neurons in pretrained transformers | 2022|  
|  | On the pitfalls of analyzing individual neurons in language models  | 2022 | 
|  | Discovering latent knowledge in language models without supervision. | 2023 |
|  | What is one grain of sand in the desert? analyzing individual neurons in deep nlp models. | 2018 | 
| EMNLP / Geva  | Transformer feed-forward layers are key-value memories. | 2021 |
| EMNLP / Geva  | Transformer feed-forward layers build predictions by promoting concepts in the vocabulary space. | 2022|
| |  Universal neurons in GPT2 language models. | 2023 | 
|  | Finding neurons in a haystack: Case studies with sparse probing.  |2023| 
