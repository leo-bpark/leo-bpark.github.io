---
layout: default
title: 'Knowledge Verification'
img: /assets/img/logos/knowledge_verification.png
description: Google 
gradient: linear-gradient(135deg, #0064e1 0%, #5bd3ff 100%)
hover-gradient: linear-gradient(135deg, #00c6fb 0%, #005bea 100%)
date: 2021-01-21

---

| Tag | Title | Year | URL | 
|---| -----| ----| ----| 
| ACL | Do Large Language Models Know What They Donâ€™t Know? | 2023 | [ACL](https://aclanthology.org/2023.findings-acl.551/) | 
|    | Do Language Models Know When They're Hallucinating References? | 2023 |
|   | Language models (mostly) know what they know | 2022 |
|   | Knowing what llms do not know: A simple yet effective self-detection method |  2023 |
|   |  Can AI assistants know what they don't know? |  2024 | 
|  | Self-knowledge guided retrieval augmentation for large language models | 2023 | 
|  | Llm self defense: By self examination, llms know they are being tricked | 2023 | 
|  | Confidence Regulation Neurons in Language Models | 2024 |
|  | Decomposing uncertainty for large language models through input clarification ensembling. | 2023 | 
|  | Probing for uncertainty in language model latent beliefs. | 2023| 
|  | How can we know when language models know? on the calibration of language models for question answering. |  2021 | 
|  | Semantic uncertainty: Linguistic invariances for uncertainty estimation in natural language generation. | 
|  | Teaching models to express their uncertainty in words. | 2022 | 
|  | Uncertainty estimation in autoregressive structured prediction. | 2022 |
|  | Emergent linear structure in large language model representations of true/false datasets, | | 
|  | Locating and editing factual associations in GPT | 2022 | 
| ICLR | Prompting GPT-3 to be reliable. | 2023 | 
|  | Just ask for calibration: Strategies for eliciting calibrated confidence scores from language models fine-tuned with human feedback. |  2023| 
| Nature | Detecting hallucinations in large language models using semantic entropy | 2024 | 
| ACL | Calibrating the Confidence of Large Language Models by Eliciting Fidelity | 2024 | 
| ACL | Fact-Level Confidence Calibration: Empowering Confidence-Guided LLM Self-Correction  | 2024 | 
