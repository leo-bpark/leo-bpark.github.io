{
    "title": "Knowledge Conflict in LLMs",
    "research-question": "조건에 따라서 규칙을 따르는 LLM에게 지식 충돌을 어떻게 전달할 수 있는가?",
    "memos":[
        "기존의 언어모델은 ",
        "기존의 언어모델은 "
    ],
    "main-contributions": [
        "Novel approach to understanding prompt injection vulnerabilities",
        "Novel approach to understanding prompt injection vulnerabilities"
    ],
    "impact": "High - addresses critical security concerns in LLM deployment",
    "highly-related-work": [
        {
            "type": "paper",
            "title": "FocalLORA",
            "memos": [
                "기존에 conflict를 다루는 방식은, 모델이 주어진 조건이 충돌이 나는 경우에 한하도록 규정한다.",
                "저자들은 LLM에 대해서 instruction과 prompt에 발생하는 conflict를 다뤘고, Attention Head에서 두 개의 차이를 측정하는 방법을 통해서, attention head의 KL divergence를 최소화하는 방법을 통해서 모델이 집중해야하는 부분에 대해서 튜닝을 진행하였다. "
            ],
            "authors": ["Zitong Shi"],
            "year": "NeurIPS 2025",
            "url": "https://openreview.net/forum?id=o2y6BS6mm0&referrer=%5Bthe%20profile%20of%20Carl%20Yang%5D(%2Fprofile%3Fid%3D%7ECarl_Yang1)"
        }
    ],
    "tags": ["knowledge", "conflict", "llm"],
    "links": [
        {
            "type": "github",
            "label": "GitHub Repository",
            "url": "https://github.com/yourusername/project"
        },
        {
            "type": "overleaf",
            "label": "Overleaf Document",
            "url": "https://www.overleaf.com/project/yourproject"
        }
    ],
    "status": "in-progress",
    "start-date": "2025-12-28",
    "last-updated": "2025-12-28",
    "highlight-color": "#00aa00",
    "progress": 60
}