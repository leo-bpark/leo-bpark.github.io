---
layout: distill-reading
language: English
category: Architecture
media: Study
bibliography: all.bib
giscus_comments: false
disqus_comments: false
date: 2025-12-08
featured: true
title: 'MIRAS (Legacy):  A Unified View of Modern Sequence Models'
description: ''
_styles: >
    .table {
        padding-top:200px;
        margin-bottom: 2.5rem;
        border-bottom: 2px;
    }
    .p {
        font-size:20px;
    }
    .styled-image {
        border-radius: 15px;
        box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);
        margin: 20px auto;
        transition: transform 0.3s ease;
        display: block;
    }
    
---



### One-Sentence Summary

**MIRAS shows that Transformers, RWKV-7, Mamba, Longhorn, DeltaNet, and many other models are not fundamentally different paradigms, but variations of a single recurrent memory update scheme, distinguished mainly by their memory representation, L2-family attentional bias, retention gates, and gradient-based update rules.**

<img src="https://d2acbkrrljl37x.cloudfront.net/bumjini-blog/study_post/MIRAS.webp" width="100%" height="auto" class="styled-image" data-zoomable/>


# MIRAS: A Unified View of Modern Sequence Models

MIRAS is a framework that shows how many recent sequence architectures  
(Transformers, Mamba, Longhorn, DeltaNet, RWKV-7, RetNet, Titans, etc.)  
can all be written as variations of the **same memory update equation**.

Instead of thinking in terms of “attention vs RNN vs SSM,”  
MIRAS describes models using five axes:

1. **Memory Architecture** (vector vs matrix vs MLP parameters)  
2. **Attentional Bias** (how keys are compared: dot-product, L2, etc.)  
3. **Retention Gate** (forget/decay mechanism)  
4. **Memory Algorithm** (nonparametric, GD, implicit GD, GD+momentum, …)  
5. **Write Operation** (how new key–value information is stored)

The table you provided is a concrete instantiation of this framework.

---

## 1. Core MIRAS Template

At a high level, all models are written as an online update of a **memory state** $M_t$:

$$
M_t
= \mathcal{A}_t(M_{t-1}) + \mathcal{W}_t(k_t, v_t),
$$

where

- $M_t$ is the memory at time $t$ (vector, matrix, or neural params),
- $k_t$ and $v_t$ are the **key** and **value** at time $t$,
- $\mathcal{A}_t$ is a **retention operator** (decay, projection, gating),
- $\mathcal{W}_t$ is a **write operator** (rank-1 write, MLP update, etc.).

Different architectures correspond to different choices for these components.

---

## 2. Memory Architecture

### 2.1 Vector vs Matrix vs Deep Memory

From the MIRAS table, models fall roughly into three categories:

| Model Examples                         | Memory Type | Description |
|----------------------------------------|------------|-------------|
| **RetNet**                             | Vector     | $M_t \in \mathbb{R}^d$ |
| **Transformer, Mamba, Longhorn, RWKV-7, DeltaNet, Gated DeltaNet, DeltaProduct** | Matrix | $M_t \in \mathbb{R}^{d \times d}$ |
| **TTT-MLP, Titans-LMM, MONETA, Yaad, Memora** | Deep / MLP | Memory implicitly stored in neural network weights |

**Vector memory** stores a single activation per channel (no explicit feature–feature interactions).  
**Matrix memory** stores **outer-product structure**, so it can capture
relations like $v_t k_t^\top$ (very similar to KV matrices in attention).  
**Deep memory** stores information implicitly inside parameterized functions (MLPs).

---

## 3. Attentional Bias: Dot-Product vs L2

MIRAS emphasizes the function class behind **how keys are compared**, rather than the literal code implementation.

### 3.1 Classical Dot-Product Attention

In vanilla Transformers, the attention score between two positions $i$ and $t$ is:

$$
s_{i,t} = \frac{k_i^\top k_t}{\sqrt{d_k}},
\qquad
\alpha_{i,t} \propto \exp(s_{i,t}).
$$

This is usually called **dot-product attention**.

### 3.2 L2-Based View of the Same Kernel

However, dot-product can be re-expressed via the squared L2 distance:

$$
\|k_i - k_t\|_2^2
= \|k_i\|_2^2 + \|k_t\|_2^2 - 2 k_i^\top k_t.
$$

Rearranging:

$$
k_i^\top k_t
= -\frac{1}{2}\|k_i - k_t\|_2^2
+ \frac{1}{2}\big(\|k_i\|_2^2 + \|k_t\|_2^2\big).
$$

When we exponentiate and normalize over $i$, the terms independent of $i$ act as a constant and cancel in the softmax. So:

$$
\exp(k_i^\top k_t)
\propto
\exp\!\Big(-\tfrac{1}{2}\|k_i - k_t\|_2^2\Big).
$$

Thus, **dot-product attention is mathematically equivalent to an L2 (Gaussian) kernel on keys**, up to constants.

### 3.3 Why MIRAS Labels Transformer Bias as “L2”

Because MIRAS groups models by the *family of kernels* they use, it treats Transformer attention as belonging to the **L2-based kernel family**:

- Implementation: dot-product in code  
- Functional family: equivalent to an L2 (RBF-like) kernel

So in the table, Transformer is placed under **“L2 attentional bias”**, even though we traditionally call it “dot-product attention.”

By contrast, some models explicitly use other bias forms (e.g., pure dot-product without L2 interpretation, $L_1$, Huber, KL, etc.), and those are labeled differently.

---

## 4. Retention Gate (Forget Gate)

The **retention gate** describes how past memory is preserved or forgotten.

Common forms in MIRAS:

### 4.1 Simple Multiplicative Decay

For some models:

$$
M_t = \alpha_t \odot M_{t-1} + \text{(write term)},
$$

where $0 \le \alpha_t \le 1$ (possibly channel-wise).  
Example: plain exponential decay.

### 4.2 L2-Based Projection Forgetting

Several recent models use **projection-based forgetting**, driven by L2 logic:

- **Mamba (2024)**  
- **Gated DeltaNet (2024)**  
- **RWKV-7 (2025)**  
- **DeltaProduct (2025)**

They employ terms like:

$$
(I - \beta_t k_t k_t^\top) M_{t-1},
$$

which:

- removes components of $M_{t-1}$ aligned with key $k_t$,
- keeps components orthogonal to $k_t$.

This is naturally derived from minimizing an **L2 regression loss**
(e.g., $\|M k_t - v_t\|_2^2$) with a gradient-descent step.

MIRAS therefore classifies these as using **L2 retention gates**.

---

## 5. Memory Algorithms: GD, Implicit GD, Nonparametric

The **Memory Algorithm** column in the table describes how the update can be interpreted:

- **Nonparametric**  
  – e.g., classical Transformers storing all KV pairs and recomputing attention at each step.

- **GD (Gradient Descent)**  
  – a single explicit gradient step on an L2-type objective.  
    Many matrix-memory models fall here: DFW, Mamba, Gated DeltaNet, RWKV-7.

- **Implicit GD**  
  – the update is equivalent to GD on some objective, but not written as an explicit gradient step (e.g., Longhorn).

- **GD + Momentum**  
  – Titans-LMM adds a momentum term on top of GD.

The surprising conclusion from the table is that **many very different-looking architectures are essentially performing online gradient descent on a local error functional**.

---

## 6. Memory Write Operations

The **Memory Write Operation** column shows how new information enters the memory state.

### 6.1 Rank-1 Writes

Most matrix-memory models write via a rank-1 update:

$$
\mathcal{W}_t(k_t, v_t) = \beta_t v_t k_t^\top,
$$

where $\beta_t$ is a data-dependent gate.

Examples:

- Mamba  
- Gated DeltaNet  
- RWKV-7  
- DeltaProduct

These resemble the **KV outer-products** accumulated in attention mechanisms.

### 6.2 Vector Writes

For vector memory, writes look like:

$$
M_t = \alpha_t M_{t-1} + v_t k_t^\top \quad\text{(collapsed to vector form)},
$$

as in RetNet’s vector memory.

### 6.3 Deep/MLP Writes

For deep-memory models (e.g., MONETA, Yaad, Memora), the write is implemented through a neural network:

$$
W_t = W_{t-1} - \eta_t \nabla \mathcal{L}(W_{t-1}; k_t, v_t),
$$

or more generally:

$$
W_t = \mathrm{MLP}(W_{t-1}, k_t, v_t),
$$

so the “memory” is the parameter vector $W_t$ itself.

---

## 7. Shallow vs Deep Memory (From the Table)

The MIRAS table partitions models into:

### 7.1 Shallow Memory

Models with a **single explicit memory state** (vector or matrix) and simple GD-like updates:

- RetNet  
- Transformer  
- LA, DFW, Lightening Attention, GLA  
- Mamba, HGRN2, DeltaNet, Longhorn, TTT-Linear  
- Gated DeltaNet, RWKV-7, DeltaProduct

They differ mainly by:

- whether the memory is vector or matrix,
- whether the bias is L2 vs dot-product,
- whether the retention gate is trivial or L2 projection.

### 7.2 Deep Memory

Models where memory is stored in **multi-layer neural parameters**:

- TTT-MLP  
- Titans-LMM  
- MONETA  
- Yaad  
- Memora

These use 2-layer or $k$-layer MLPs and typically operate directly on gradient updates of a parameter matrix, with losses based on $L_p$, Huber, KL, etc.

---

## 8. RWKV-7 in the MIRAS View

RWKV-7 is highlighted in the table as:

- **Memory Architecture:** Matrix  
- **Attentional Bias:** L2  
- **Retention Gate:** L2  
- **Memory Algorithm:** GD  
- **Write Operation:**  
  $$
  M_t = \mathrm{diag}(\alpha_t) (I - \beta_t k_t k_t^\top) M_{t-1}
        + \beta_t v_t k_t^\top.
  $$

Interpretation:

- It maintains a matrix memory $M_t$.
- It applies channel-wise decay via $\alpha_t$.
- It performs L2-motivated projection forgetting via $(I - \beta_t k_t k_t^\top)$.
- It writes the new key–value interaction via a rank-1 update $\beta_t v_t k_t^\top$.
- This is exactly one GD step on the L2 loss $\|M k_t - v_t\|_2^2$.

RWKV-7 thus sits at the intersection of **RNN recurrence**, **matrix memory**, and **L2-based attention**, all within the MIRAS template.

---

## 9. Transformer in the MIRAS View (Clarified)

Even though traditional terminology calls Transformer attention **dot-product**,  
MIRAS places it under **L2 attentional bias** because:

- Its kernel  
  $$
  \exp(k_i^\top k_t)
  $$
  is equivalent (modulo constants) to  
  $$
  \exp\!\left(-\tfrac{1}{2}\|k_i - k_t\|_2^2\right),
  $$
  which is an L2 Gaussian kernel.

Therefore, in MIRAS:

- **Implementation perspective:** Transformer uses dot-product attention.  
- **Kernel-family perspective:** Transformer uses an **L2-derived kernel**.  

This is why the table lists **L2** in the “Attentional Bias” column for Transformer.

---

## 10. Conceptual Takeaways

1. **Unified Template**  
   Most modern sequence models are just different instantiations of:
   $$
   M_t = \mathcal{A}_t(M_{t-1}) + \mathcal{W}_t(k_t, v_t),
   $$
   with specific choices for memory type, bias, gate, and optimization rule.

2. **Attention vs RNN vs SSM is a false dichotomy**  
   Many “new architectures” (Mamba, Longhorn, RWKV-7, DeltaNet, etc.) are essentially **RNNized attention with L2-style objectives and GD updates**.

3. **Dot-Product vs L2 is mostly a matter of viewpoint**  
   At the kernel level, dot-product attention lives in the same family as L2 Gaussian kernels, which is why MIRAS consistently labels attentional bias as **L2** for Transformers and several newer models.

4. **Matrix memory is becoming the default**  
   Matrix-valued states (as in Mamba, RWKV-7, DeltaProduct) give attention-like expressiveness while preserving recurrent efficiency.

---
