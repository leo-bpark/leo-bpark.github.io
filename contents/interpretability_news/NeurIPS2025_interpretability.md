---
layout: distill
bibliography: all.bib
giscus_comments: false
disqus_comments: false
date: 2025-11-27
featured: true
img: assets/img/feigenbaum.png
title: 'NeurIPS 2026 Interpretability Papers'
description: ''
_styles: >
    .pioneer-container {
        background: #000;
        color: #fff;
        font-family: 'Times New Roman', serif;
        padding: 2rem;
        max-width: 1200px;
        margin: 0 auto;
    }
    .pioneer-header {
        display: flex;
        justify-content: space-between;
        align-items: center;
        margin-bottom: 2rem;
        padding-bottom: 1rem;
        border-bottom: 1px solid #d4af37;
    }
    .pioneer-title {
        font-size: 1.5rem;
        font-weight: normal;
        margin: 0;
    }
    .pioneer-subtitle {
        font-size: 1.5rem;
        font-weight: normal;
        margin: 0;
        position: relative;
    }
    .pioneer-subtitle::after {
        content: '';
        position: absolute;
        bottom: -1rem;
        left: 50%;
        transform: translateX(-50%);
        width: 20px;
        height: 2px;
        background: #d4af37;
        border-radius: 1px;
    }
    .pioneer-intro {
        font-size: 1.1rem;
        line-height: 1.6;
        margin-bottom: 3rem;
        text-align: left;
    }
    .pioneer-table {
        width: 100%;
        border-collapse: collapse;
    }
    .pioneer-table th {
        text-align: left;
        font-weight: normal;
        font-size: 1rem;
        padding-bottom: 1rem;
        border-bottom: 1px solid #d4af37;
        color: #fff;
    }
    .pioneer-table td {
        padding: 0.8rem 0;
        border-bottom: 1px solid #333;
        font-size: 0.95rem;
        vertical-align: top;
    }
    .pioneer-table tr:hover {
        background: rgba(212, 175, 55, 0.05);
    }
    .pioneer-date {
        width: 15%;
        font-variant-numeric: tabular-nums;
        padding-right: 2rem;
    }
    .pioneer-title-col {
        width: 60%;
        font-weight: 500;
    }
    .pioneer-media {
        width: 25%;
        color: #d4af37;
        text-transform: uppercase;
        font-size: 0.85rem;
        letter-spacing: 0.5px;
    }
    .pioneer-link {
        color: #fff;
        text-decoration: none;
    }
    .pioneer-link:hover {
        color: #d4af37;
        text-decoration: underline;
    }
---





## References

[OpenReview NeurIPS](https://openreview.net/group?id=NeurIPS.cc/2025/Conference#tab-accept-oral)

1. **GnnXemplar: Exemplars to Explanations - Natural Language Rules for Global GNN Interpretability** — Burouj Armgaan et al.
2. **A is for Absorption: Studying Feature Splitting and Absorption in Sparse Autoencoders** — David Chanin et al.
3. **Identifiability of Deep Polynomial Neural Networks** — Konstantin Usevich et al.
4. **A Snapshot of Influence: A Local Data Attribution Framework for Online Reinforcement Learning** — Yuzheng Hu et al.
5. **Proxy-SPEX: Sample-Efficient Interpretability via Sparse Feature Interactions in LLMs** — Landon Butler et al.
6. **The Non-Linear Representation Dilemma: Is Causal Abstraction Enough for Mechanistic Interpretability?** — Denis Sutter et al.
7. **Towards Interpretable and Efficient Attention: Compressing All by Contracting a Few** — Qishuai Wen et al.
8. **Measuring and Guiding Monosemanticity** — Ruben Härle et al.
9. **Improved Representation Steering for Language Models** — Zhengxuan Wu et al.
10. **Head Pursuit: Probing Attention Specialization in Multimodal Transformers** — Lorenzo Basile et al.
11. **How do Transformers Learn Implicit Reasoning?** — Jiaran Ye et al.
12. **Jacobian-Based Interpretation of Nonlinear Neural Encoding Model** — Xiaohui Gao et al.
13. **What One Cannot, Two Can: Two-Layer Transformers Provably Represent Induction Heads on Any-Order Markov Chains** — Chanakya Ekbote et al.
14. **Controlling Thinking Speed in Reasoning Models** — Zhengkai Lin et al.
15. **Minimax-Optimal Univariate Function Selection in Sparse Additive Models: Rates, Adaptation, and the Estimation-Selection Gap** — Shixiang Liu et al.
16. **The Structure of Relation Decoding Linear Operators in Large Language Models** — Miranda Anna Christ et al.
17. **SHAP values via sparse Fourier representation** — Ali Gorji et al.
18. **Self-Assembling Graph Perceptrons** — Jialong Chen et al.
19. **DeepHalo: A Neural Choice Model with Controllable Context Effects** — Shuhan Zhang et al.
20. **Disentangled Concepts Speak Louder Than Words: Explainable Video Action Recognition** — Jongseo Lee et al.
21. **ARECHO: Autoregressive Evaluation via Chain-Based Hypothesis Optimization for Speech Multi-Metric Estimation** — Jiatong Shi et al.
22. **Differentiable Decision Tree via "ReLU+Argmin" Reformulation** — Qiangqiang Mao et al.
23. **The Fragile Truth of Saliency: Improving LLM Input Attribution via Attention Bias Optimization** — Yihua Zhang et al.
24. **Transferring Linear Features Across Language Models With Model Stitching** — Alan Chen et al.
25. **Does Object Binding Naturally Emerge in Large Pretrained Vision Transformers?** — Yihao Li et al.
26. **Emergence and Evolution of Interpretable Concepts in Diffusion Models** — Berk Tinaz et al.
27. **A Implies B: Circuit Analysis in LLMs for Propositional Logical Reasoning** — Guan Zhe Hong et al.
28. **GaussianFusion: Gaussian-Based Multi-Sensor Fusion for End-to-End Autonomous Driving** — Shuai Liu et al.
29. **GraphMaster: Automated Graph Synthesis via LLM Agents in Data-Limited Environments** — Enjun Du et al.
30. **Q-Insight: Understanding Image Quality via Visual Reinforcement Learning** — Weiqi Li et al.
31. **Revisiting Generative Infrared and Visible Image Fusion Based on Human Cognitive Laws** — Lin Guo et al.
32. **Vision Transformers Don't Need Trained Registers** — Nicholas Jiang et al.
33. **Provable Gradient Editing of Deep Neural Networks** — Zhe Tao et al.
34. **Decomposing stimulus-specific sensory neural information via diffusion models** — Steeve Laquitaine et al.